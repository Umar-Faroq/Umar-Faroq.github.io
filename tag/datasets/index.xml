<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Datasets | Min-Hung Chen</title><link>https://minhungchen.netlify.app/tag/datasets/</link><atom:link href="https://minhungchen.netlify.app/tag/datasets/index.xml" rel="self" type="application/rss+xml"/><description>Datasets</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Min-Hung Chen</copyright><lastBuildDate>Sun, 20 Jun 2021 08:50:00 +0000</lastBuildDate><image><url>https://minhungchen.netlify.app/img/authors/head_me.jpg</url><title>Datasets</title><link>https://minhungchen.netlify.app/tag/datasets/</link></image><item><title>Learned Smartphone ISP Challenge</title><link>https://minhungchen.netlify.app/talk/mai_cvpr21/</link><pubDate>Sun, 20 Jun 2021 08:50:00 +0000</pubDate><guid>https://minhungchen.netlify.app/talk/mai_cvpr21/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click on the &lt;strong>Slides&lt;/strong> button above to view the built-in slides feature.
&lt;/div>
&lt;/div>
Slides can be added in a few ways:
- **Create** slides using Academic's [*Slides*](https://sourcethemes.com/academic/docs/managing-content/#create-slides) feature and link using `slides` parameter in the front matter of the talk file
- **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file
- **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://sourcethemes.com/academic/docs/writing-markdown-latex/).
Further talk details can easily be added to this page using *Markdown* and $\rm \LaTeX$ math code. -->
&lt;!-- Feel free to click the upper buttons `Video` and `Slides` for more details. -->
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/tdpmvy2Cab0?start=7941&amp;amp;end=8600" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>All the links in this talk are as follows:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://github.com/MediaTek-NeuroPilot/mai21-learned-smartphone-isp" target="_blank" rel="noopener">Codebase&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://arxiv.org/abs/2105.07809" target="_blank" rel="noopener">Paper&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://github.com/MediaTek-NeuroPilot/tflite-neuron-delegate" target="_blank" rel="noopener">Neuron Delegate&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://www.mediatek.com/products/smartphones/dimensity-1000-series" target="_blank" rel="noopener">Dimensity 1000+&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://careers.mediatek.com/" target="_blank" rel="noopener">Job Opportunities&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Learned Smartphone ISP on Mobile NPUs With Deep Learning, Mobile AI 2021 Challenge: Report</title><link>https://minhungchen.netlify.app/publication/mai/</link><pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/mai/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="resources">Resources&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-paper-arxivhttpsarxivorgpdf210507809pdf">
&lt;a href="https://arxiv.org/pdf/2105.07809.pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="15%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/2105.07809.pdf">Paper (arXiv)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ignatov_Learned_Smartphone_ISP_on_Mobile_NPUs_With_Deep_Learning_Mobile_CVPRW_2021_paper.html" target="_blank" rel="noopener">CVF Open Access&lt;/a>&lt;/li>
&lt;li>
&lt;a href="">IEEE Xplore (to be updated)&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://ai-benchmark.com/workshops/mai/2021/" target="_blank" rel="noopener">Mobile AI (MAI) Workshop @ CVPR 2021&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://competitions.codalab.org/competitions/28054" target="_blank" rel="noopener">Learned Smartphone ISP Challenge @ MAI 2021&lt;/a>&lt;/li>
&lt;li>MediaTek Blog [
&lt;a href="https://www.mediatek.com/blog/mediateks-summer-of-ai-at-cvpr-2021" target="_blank" rel="noopener">Blog-1&lt;/a> ][
&lt;a href="https://www.mediatek.com/blog/mediatek-hosts-cvpr-and-icmr-challenges-sign-up-today" target="_blank" rel="noopener">Blog-2&lt;/a> ]&lt;/li>
&lt;li>
&lt;a href="https://www.mediatek.com/products/smartphones/dimensity-1000-series" target="_blank" rel="noopener">MediaTek Dimensity 1000+ APU&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;ul>
&lt;li>
&lt;a href="https://scholar.google.com/citations?user=kBoWvhIAAAAJ" target="_blank" rel="noopener">Andrey Ignatov&lt;/a>,
&lt;a href="https://scholar.google.com/citations?user=CyMTGlsAAAAJ" target="_blank" rel="noopener">Cheng-Ming Chiang&lt;/a>,
&lt;a href="https://scholar.google.com/citations?user=EeVsWaUAAAAJ" target="_blank" rel="noopener">Hsien-Kai Kuo&lt;/a>, Anastasia Sycheva,
&lt;a href="https://people.ee.ethz.ch/~timofter/" target="_blank" rel="noopener">Radu Timofte&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://www.linkedin.com/in/man-yu-lee-b207471aa/" target="_blank" rel="noopener">Man-Yu Lee&lt;/a>,
&lt;a href="https://xusean0118.github.io/" target="_blank" rel="noopener">Yu-Syuan Xu&lt;/a>, Yu Tseng, et al. &amp;ldquo;Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021 Challenge: Report&amp;rdquo;,
&lt;a href="https://ai-benchmark.com/workshops/mai/2021/" target="_blank" rel="noopener">&lt;em>CVPR Workshop and Challenges on Mobile AI (MAI)&lt;/em>, 2021&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{ignatov2021learned,
title={Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021 Challenge: Report},
author={Ignatov, Andrey and Chiang, Cheng-Ming and Kuo, Hsien-Kai and Sycheva, Anastasia and Timofte, Radu and Chen, Min-Hung and Lee, Man-Yu and Xu, Yu-Syuan and Tseng, Yu and Xu, Shusong and Guo, Jin and Chen, Chao-Hung and Hsyu, Ming-Chun and Tsai, Wen-Chia and Chen, Chao-Wei and Malivenko, Grigory and Kwon, Minsu and Lee, Myungje and Yoo, Jaeyoon and Kang, Changbeom and Wang, Shinjo and Shaolong, Zheng and Dejun, Hao and Fen, Xie and Zhuang, Feng and Ma, Yipeng and Peng, Jingyang and Wang, Tao and Song, Fenglong and Hsu, Chih-Chung and Chen, Kwan-Lin and Wu, Mei-Hsuang and Chudasama, Vishal and Prajapati, Kalpesh and Patel, Heena and Sarvaiya, Anjali and Upla, Kishor and Raja, Kiran and Ramachandra, Raghavendra and Busch, Christoph and de Stoutz, Etienne},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops (Mobile AI)},
year={2021},
url={https://arxiv.org/abs/2105.07809}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>MediaTek Inc.   &lt;sup>2&lt;/sup>ETH Zurich &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=kBoWvhIAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-andrey-ignatovsup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ai.jpg" alt="" width="100%" >
&lt;figcaption>
Andrey Ignatov&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=CyMTGlsAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-cheng-ming-chiangsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_cmc.jpg" alt="" width="100%" >
&lt;figcaption>
Cheng-Ming Chiang&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=EeVsWaUAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-hsien-kai-kuosup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_hkk.jpg" alt="" width="100%" >
&lt;figcaption>
Hsien-Kai Kuo&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="">
&lt;figure id="figure-anastasia-sychevasup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Anastasia Sycheva&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://people.ee.ethz.ch/~timofter/" target="_blank" rel="noopener">
&lt;figure id="figure-radu-timoftesup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_rt.jpg" alt="" width="100%" >
&lt;figcaption>
Radu Timofte&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/man-yu-lee-b207471aa/" target="_blank" rel="noopener">
&lt;figure id="figure-man-yu-leesup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_myl.jpg" alt="" width="100%" >
&lt;figcaption>
Man-Yu Lee&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://xusean0118.github.io/" target="_blank" rel="noopener">
&lt;figure id="figure-yu-syuan-xusup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ysx.jpg" alt="" width="100%" >
&lt;figcaption>
Yu-Syuan Xu&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="">
&lt;figure id="figure-yu-tsengsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Yu Tseng&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Deep Learning for Smartphone ISP</title><link>https://minhungchen.netlify.app/project/mai/</link><pubDate>Sun, 16 May 2021 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/project/mai/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>As the popularity of mobile photography is growing constantly, lots of efforts are being invested now into building complex hand-crafted camera ISP systems. Despite the advances in this field, the modern smartphone cameras are still unable to reach the level of the high-end DSLR cameras, partly because the conventional ISP algorithms are unable to accurately recover the information coming from the tiny mobile camera sensors or render the missing image details. To address this problem, we organize this challenge and present a large-scale dataset consisting of aligned smartphone raw / DSLR image pairs to train deep learning-based ISPs.&lt;/p>
&lt;hr>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;p>The challenge aims to obtain an output image with the best trade-off between pixel fidelity (PSNR) and latency on mobile devices.&lt;/p>
&lt;hr>
&lt;h2 id="our-approaches">Our Approaches&lt;/h2>
&lt;p>The whole pipeline of &lt;strong>Learned Smartphone ISP&lt;/strong> has two main steps (assume the input resolution is &lt;code>H x W&lt;/code>):&lt;/p>
&lt;ol>
&lt;li>deBayer pre-processing:
&lt;ul>
&lt;li>&lt;em>Input&lt;/em>: RAW data [&lt;code>H x W x 1&lt;/code>]&lt;/li>
&lt;li>&lt;em>Output&lt;/em>: deBayer RAW data [&lt;code>(H/2) x (W/2) x 4&lt;/code>]&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>We propose an UNet-like architecture, &lt;strong>PUNET&lt;/strong>, to take deBayer data and output RGB images:
&lt;ul>
&lt;li>&lt;em>Input&lt;/em>: deBayer RAW data [&lt;code>(H/2) x (W/2) x 4&lt;/code>]&lt;/li>
&lt;li>&lt;em>Output&lt;/em>: RGB image [&lt;code>H x W x 3&lt;/code>]&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;figure id="figure-punet-architecture">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/mai/PUNET_hud38395a945c59de4280691f2b874f3e5_81589_2000x2000_fit_lanczos_2.png" data-caption="PUNET architecture.">
&lt;img data-src="https://minhungchen.netlify.app/project/mai/PUNET_hud38395a945c59de4280691f2b874f3e5_81589_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="1011">
&lt;/a>
&lt;figcaption>
PUNET architecture.
&lt;/figcaption>
&lt;/figure>
&lt;p>A detailed description of the entire baseline training pipeline can be found in our GitHub: &lt;a href="https://github.com/MediaTek-NeuroPilot/mai21-learned-smartphone-isp">https://github.com/MediaTek-NeuroPilot/mai21-learned-smartphone-isp&lt;/a>.&lt;/p>
&lt;hr>
&lt;h2 id="dataset">Dataset&lt;/h2>
&lt;p>(Will be released. Stay tuned.)&lt;/p>
&lt;p>More information of the dataset can be found in our GitHub: &lt;a href="https://github.com/MediaTek-NeuroPilot/mai21-learned-smartphone-isp">https://github.com/MediaTek-NeuroPilot/mai21-learned-smartphone-isp&lt;/a>.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>We evaluate the pre-trained PUNET on the validation data (resolution: 256x256), and obtain the following results:&lt;/p>
&lt;ul>
&lt;li>PSNR: &lt;strong>23.03&lt;/strong>&lt;/li>
&lt;li>Some visualized comparison with the ground truth:
&lt;figure id="figure-baseline-results-visualization">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/mai/PUNET_results_patch_hu6ae68bdca1a543daaf9d0a80aff5a2e1_1013124_2000x2000_fit_lanczos_2.png" data-caption="Baseline results visualization.">
&lt;img data-src="https://minhungchen.netlify.app/project/mai/PUNET_results_patch_hu6ae68bdca1a543daaf9d0a80aff5a2e1_1013124_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="605">
&lt;/a>
&lt;figcaption>
Baseline results visualization.
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;!-- Please check our [paper]() for more results. -->
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;h4 id="papers--code">Papers &amp;amp; Code&lt;/h4>
&lt;p>
&lt;a href="https://github.com/MediaTek-NeuroPilot/mai21-learned-smartphone-isp" target="_blank" rel="noopener">
&lt;figure id="figure-codehttpsgithubcommediatek-neuropilotmai21-learned-smartphone-isp">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;figcaption>
&lt;a href="https://github.com/MediaTek-NeuroPilot/mai21-learned-smartphone-isp">&lt;strong>Code&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/p>
&lt;p>
&lt;a href="https://arxiv.org/pdf/2105.07809.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-cvprw21httpsarxivorgpdf210507809pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="15%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/2105.07809.pdf">CVPRW'21&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/p>
&lt;h4 id="other-links">Other Links&lt;/h4>
&lt;ul>
&lt;li>
&lt;a href="https://ai-benchmark.com/workshops/mai/2021/" target="_blank" rel="noopener">Mobile AI (MAI) Workshop @ CVPR 2021&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://competitions.codalab.org/competitions/28054" target="_blank" rel="noopener">Learned Smartphone ISP Challenge @ MAI 2021&lt;/a>&lt;/li>
&lt;li>MediaTek Blog [
&lt;a href="https://www.mediatek.com/blog/mediateks-summer-of-ai-at-cvpr-2021" target="_blank" rel="noopener">Blog-1&lt;/a> ][
&lt;a href="https://www.mediatek.com/blog/mediatek-hosts-cvpr-and-icmr-challenges-sign-up-today" target="_blank" rel="noopener">Blog-2&lt;/a> ]&lt;/li>
&lt;li>
&lt;a href="https://www.mediatek.com/products/smartphones/dimensity-1000-series" target="_blank" rel="noopener">MediaTek Dimensity 1000+ APU&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="related-publications">Related Publications&lt;/h2>
&lt;p>If you find this project useful, please cite our work:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://scholar.google.com/citations?user=kBoWvhIAAAAJ" target="_blank" rel="noopener">Andrey Ignatov&lt;/a>,
&lt;a href="https://scholar.google.com/citations?user=CyMTGlsAAAAJ" target="_blank" rel="noopener">Cheng-Ming Chiang&lt;/a>,
&lt;a href="https://scholar.google.com/citations?user=EeVsWaUAAAAJ" target="_blank" rel="noopener">Hsien-Kai Kuo&lt;/a>, Anastasia Sycheva,
&lt;a href="https://people.ee.ethz.ch/~timofter/" target="_blank" rel="noopener">Radu Timofte&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://www.linkedin.com/in/man-yu-lee-b207471aa/" target="_blank" rel="noopener">Man-Yu Lee&lt;/a>,
&lt;a href="https://xusean0118.github.io/" target="_blank" rel="noopener">Yu-Syuan Xu&lt;/a>, Yu Tseng, et al. &amp;ldquo;Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021 Challenge: Report&amp;rdquo;,
&lt;a href="https://ai-benchmark.com/workshops/mai/2021/" target="_blank" rel="noopener">&lt;em>CVPR Workshop and Challenges on Mobile AI (MAI)&lt;/em>, 2021&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@misc{mtk2021mai,
title={Mobile AI Workshop: Learned Smartphone ISP Challenge},
year={2021},
url={https://github.com/MediaTek-NeuroPilot/mai21-learned-smartphone-isp}
}
@inproceedings{ignatov2021learned,
title={Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021 Challenge: Report},
author={Ignatov, Andrey and Chiang, Cheng-Ming and Kuo, Hsien-Kai and Sycheva, Anastasia and Timofte, Radu and Chen, Min-Hung and Lee, Man-Yu and Xu, Yu-Syuan and Tseng, Yu and Xu, Shusong and Guo, Jin and Chen, Chao-Hung and Hsyu, Ming-Chun and Tsai, Wen-Chia and Chen, Chao-Wei and Malivenko, Grigory and Kwon, Minsu and Lee, Myungje and Yoo, Jaeyoon and Kang, Changbeom and Wang, Shinjo and Shaolong, Zheng and Dejun, Hao and Fen, Xie and Zhuang, Feng and Ma, Yipeng and Peng, Jingyang and Wang, Tao and Song, Fenglong and Hsu, Chih-Chung and Chen, Kwan-Lin and Wu, Mei-Hsuang and Chudasama, Vishal and Prajapati, Kalpesh and Patel, Heena and Sarvaiya, Anjali and Upla, Kishor and Raja, Kiran and Ramachandra, Raghavendra and Busch, Christoph and de Stoutz, Etienne},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops (Mobile AI)},
year={2021},
url={https://arxiv.org/abs/2105.07809}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>MediaTek Inc.   &lt;sup>2&lt;/sup>ETH Zurich &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=kBoWvhIAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-andrey-ignatovsup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ai.jpg" alt="" width="100%" >
&lt;figcaption>
Andrey Ignatov&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=CyMTGlsAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-cheng-ming-chiangsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_cmc.jpg" alt="" width="100%" >
&lt;figcaption>
Cheng-Ming Chiang&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=EeVsWaUAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-hsien-kai-kuosup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_hkk.jpg" alt="" width="100%" >
&lt;figcaption>
Hsien-Kai Kuo&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="">
&lt;figure id="figure-anastasia-sychevasup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Anastasia Sycheva&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://people.ee.ethz.ch/~timofter/" target="_blank" rel="noopener">
&lt;figure id="figure-radu-timoftesup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_rt.jpg" alt="" width="100%" >
&lt;figcaption>
Radu Timofte&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/man-yu-lee-b207471aa/" target="_blank" rel="noopener">
&lt;figure id="figure-man-yu-leesup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_myl.jpg" alt="" width="100%" >
&lt;figcaption>
Man-Yu Lee&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://xusean0118.github.io/" target="_blank" rel="noopener">
&lt;figure id="figure-yu-syuan-xusup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ysx.jpg" alt="" width="100%" >
&lt;figcaption>
Yu-Syuan Xu&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="">
&lt;figure id="figure-yu-tsengsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Yu Tseng&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Temporal Attentive Alignment for Video Domain Adaptation</title><link>https://minhungchen.netlify.app/project/cdar/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/project/cdar/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Furthermore, there do not exist well-organized datasets to evaluate and benchmark the performance of DA algorithms for videos. Therefore, new datasets and approaches for video DA are desired.&lt;/p>
&lt;hr>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;p>Videos can suffer from domain shift along both the spatial and temporal directions, bringing the need of alignment for embedded feature spaces along both directions. However, most DA approaches focus on spatial direction only.&lt;/p>
&lt;hr>
&lt;h2 id="our-approaches">Our Approaches&lt;/h2>
&lt;p>We propose &lt;strong>Temporal Attentive Adversarial Adaptation Network (TA&lt;sup>3&lt;/sup>N)&lt;/strong> to simultaneously attend, align and learn temporal dynamics across domains. TA&lt;sup>3&lt;/sup>N contains the following components:&lt;/p>
&lt;ul>
&lt;li>&lt;em>TemRelation module&lt;/em>: learn various local temporal features embedded with relation information in different temporal scales&lt;/li>
&lt;li>&lt;em>Domain attention mechanism&lt;/em>: align local features with larger domain discrepancy (i.e. contribute more to the overall domain shift)
&lt;ul>
&lt;li>Assign local features with attention weights calculated using domain entropy&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;figure id="figure-the-domain-attention-mechanism-in-tasup3supn-thicker-arrows-correspond-to-larger-attention-weights">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/approach_TA3N_huff70fa82cfaf62901a8cb74823d96b36_122314_2000x2000_fit_lanczos_2.png" data-caption="The domain attention mechanism in TA3N (Thicker arrows correspond to larger attention weights).">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/approach_TA3N_huff70fa82cfaf62901a8cb74823d96b36_122314_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="100%" height="346">
&lt;/a>
&lt;figcaption>
The domain attention mechanism in TA&lt;sup>3&lt;/sup>N (Thicker arrows correspond to larger attention weights).
&lt;/figcaption>
&lt;/figure>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>We evaluate both small- and large-scale datasets, and achieve the state-of-the-art performance:&lt;/p>
&lt;ul>
&lt;li>Quantitative results:
&lt;figure id="figure-the-comparison-of-accuracy--with-other-approaches-on-_ucf-hmdbsubfullsub_-ucf----hmdb">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/result_TA3N_quantitative_hu86af7cd341c815527e27bee7cb48f53f_82705_2000x2000_fit_lanczos_2.png" data-caption="The comparison of accuracy (%) with other approaches on UCF-HMDBfull (UCF &amp;ndash;&amp;gt; HMDB).">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/result_TA3N_quantitative_hu86af7cd341c815527e27bee7cb48f53f_82705_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="50%" height="360">
&lt;/a>
&lt;figcaption>
The comparison of accuracy (%) with other approaches on &lt;em>UCF-HMDB&lt;sub>full&lt;/sub>&lt;/em> (UCF &amp;ndash;&amp;gt; HMDB).
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;li>Qualitative results:
&lt;figure id="figure-the-comparison-of-t-sne-visualization-between-image-based-da-and-our-approach-blue-source-orange-target">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/result_TA3N_qualitative_hu52c64adb84bea465dcafc27d3d518cd8_157293_2000x2000_fit_lanczos_2.png" data-caption="The comparison of t-SNE visualization between image-based DA and our approach (blue: source, orange: target).">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/result_TA3N_qualitative_hu52c64adb84bea465dcafc27d3d518cd8_157293_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="391">
&lt;/a>
&lt;figcaption>
The comparison of t-SNE visualization between image-based DA and our approach (blue: source, orange: target).
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;p>Please check our
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf" target="_blank" rel="noopener">paper&lt;/a> for more results.&lt;/p>
&lt;hr>
&lt;h2 id="videos">Videos&lt;/h2>
&lt;p>ICCV'19 Oral Presentation (please turn on closed captions):
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/j9cDuzmpYP8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>ICCV'19 Oral Presentation (officially recorded):
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8oUPyhwzIDo?start=6100&amp;amp;end=6395" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;h4 id="papers--code">Papers &amp;amp; Code&lt;/h4>
&lt;p>
&lt;a href="https://github.com/cmhungsteve/TA3N" target="_blank" rel="noopener">
&lt;figure id="figure-codehttpsgithubcomcmhungsteveta3n">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;figcaption>
&lt;a href="https://github.com/cmhungsteve/TA3N">&lt;strong>Code&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-iccv19httpsarxivorgpdf190712743pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="20%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf">ICCV'19&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://arxiv.org/pdf/1905.10861.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-cvprw19httpsarxivorgpdf190510861pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="20%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1905.10861.pdf">CVPRW'19&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;h4 id="presentations">Presentations&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://www.dropbox.com/s/7p1u8yro9hqseac/Oral_TA3N_ICCV_2019_mute.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-slides-iccv19">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/slides_ICCV2019_hubefd42348ee7371b35905da2500f1e83_82332_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="100%" height="720">
&lt;figcaption>
Slides (ICCV'19)
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.dropbox.com/s/nsb9bdfaz03mjv2/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-poster-iccv19">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/poster_ICCV2019_hub6005853782b9081f07ee0575514bfea_1443480_2000x2000_fit_lanczos_2.png" data-caption="Poster (ICCV&amp;#39;19)">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/poster_ICCV2019_hub6005853782b9081f07ee0575514bfea_1443480_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="70%" height="1776">
&lt;/a>
&lt;figcaption>
Poster (ICCV'19)
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;h4 id="other-links">Other Links&lt;/h4>
&lt;ul>
&lt;li>CVF Open Access
[
&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.html" target="_blank" rel="noopener">ICCV'19&lt;/a> ]&lt;/li>
&lt;li>IEEE Xplore
[
&lt;a href="https://ieeexplore.ieee.org/document/9008391" target="_blank" rel="noopener">ICCV'19&lt;/a> ]&lt;/li>
&lt;li>The workshop on Learning from Unlabeled Videos (LUV)
[
&lt;a href="https://sites.google.com/view/luv2019/program" target="_blank" rel="noopener">CVPRW'19&lt;/a> ]&lt;/li>
&lt;li>ML@GT [
&lt;a href="https://mlatgt.blog/2019/09/10/overcoming-large-scale-annotation-requirements-for-understanding-videos-in-the-wild" target="_blank" rel="noopener">Blog&lt;/a> ][
&lt;a href="https://mailchi.mp/4ed0cbf6a67d/iccv2019" target="_blank" rel="noopener">Article&lt;/a> ]&lt;/li>
&lt;/ul>
&lt;h4 id="datasets">Datasets&lt;/h4>
&lt;p>We propose two large-scale cross-domain action recognition datasets, &lt;em>UCF-HMDB&lt;sub>full&lt;/sub>&lt;/em> and &lt;em>Kinetics-Gameplay&lt;/em>.&lt;/p>
&lt;p>
&lt;figure id="figure-the-snapshots-in-the-_ucf-hmdbsubfullsub_-dataset">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/ucf_hmdb_hu5f00b314049d434c65c8e828bec18a41_384767_2000x2000_fit_lanczos_2.png" data-caption="The snapshots in the UCF-HMDBfull dataset.">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/ucf_hmdb_hu5f00b314049d434c65c8e828bec18a41_384767_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="589" height="50%">
&lt;/a>
&lt;figcaption>
The snapshots in the &lt;em>UCF-HMDB&lt;sub>full&lt;/sub>&lt;/em> dataset.
&lt;/figcaption>
&lt;/figure>
&lt;figure id="figure-the-snapshots-in-the-_kinetics-gameplay_-dataset">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/kinetics_gameplay_hu809b38b434046d8cf813c72d107bbaef_462778_2000x2000_fit_lanczos_2.png" data-caption="The snapshots in the Kinetics-Gameplay dataset.">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/kinetics_gameplay_hu809b38b434046d8cf813c72d107bbaef_462778_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="791" height="50%">
&lt;/a>
&lt;figcaption>
The snapshots in the &lt;em>Kinetics-Gameplay&lt;/em> dataset.
&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>To download the dataset, please visit our
&lt;a href="https://github.com/cmhungsteve/TA3N#dataset-preparation" target="_blank" rel="noopener">GitHub&lt;/a>. &lt;br>
Feel free to check our
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf" target="_blank" rel="noopener">paper&lt;/a> for more dataset details.&lt;/p>
&lt;hr>
&lt;h2 id="related-publications">Related Publications&lt;/h2>
&lt;p>If you find this project useful, please cite our papers:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen, and Jian Zheng, &amp;ldquo;Temporal Attentive Alignment for Large-Scale Video Domain Adaptation&amp;rdquo;,
&lt;a href="http://iccv2019.thecvf.com/" target="_blank" rel="noopener">&lt;em>IEEE International Conference on Computer Vision (ICCV)&lt;/em>, 2019&lt;/a> &lt;strong>[Oral (acceptance rate: 4.6%), travel grant awarded]&lt;/strong>.&lt;/li>
&lt;li>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, Zsolt Kira, and Ghassan AlRegib, &amp;ldquo;Temporal Attentive Alignment for Video Domain Adaptation&amp;rdquo;,
&lt;a href="https://sites.google.com/view/luv2019" target="_blank" rel="noopener">&lt;em>CVPR Workshop on Learning from Unlabeled Videos (LUV)&lt;/em>, 2019&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{chen2019temporal,
title={Temporal attentive alignment for large-scale video domain adaptation},
author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan and Yoo, Jaekwon and Chen, Ruxin and Zheng, Jian},
booktitle={IEEE International Conference on Computer Vision (ICCV)},
year={2019}
}
@article{chen2019taaan,
title={Temporal Attentive Alignment for Video Domain Adaptation},
author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan},
journal={CVPR Workshop on Learning from Unlabeled Videos},
year={2019},
url={https://arxiv.org/abs/1905.10861}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>Georgia Institute of Technology   &lt;sup>2&lt;/sup>Sony Interactive Entertainment LLC   &lt;sup>3&lt;/sup>Binghamton University &lt;br>&lt;/strong>
*work partially done as a SIE intern&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">
&lt;figure id="figure-zsolt-kirasup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_zk.jpg" alt="" width="100%" >
&lt;figcaption>
Zsolt Kira&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregibsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/jaekwon-yoo-8685862b/" target="_blank" rel="noopener">
&lt;figure id="figure-jaekwon-yoosup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Jaekwon Yoo&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/ruxin-chen-991477119/" target="_blank" rel="noopener">
&lt;figure id="figure-ruxin-chensup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Ruxin Chen&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?hl=en&amp;amp;user=5YR6dTEAAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate" target="_blank" rel="noopener">
&lt;figure id="figure-jian-zhengsup3sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_jz.jpg" alt="" width="100%" >
&lt;figcaption>
Jian Zheng&lt;sup>3&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Traffic Sign Detection under Challenging Conditions</title><link>https://minhungchen.netlify.app/project/curetsd/</link><pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/project/curetsd/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Existing traffic sign datasets are limited in terms of type and severity of challenging conditions. Metadata corresponding to these conditions are unavailable and it is not possible to investigate the effect of a single factor because of simultaneous changes in numerous conditions. Therefore, we introduce the &lt;strong>CURE-TSD&lt;/strong> dataset, including various challenging conditions with both real and synthetic data.&lt;/p>
&lt;!-- ---
## Challenges
(coming soon)
---
## Our Approaches
(coming soon)
-->
&lt;hr>
&lt;h2 id="dataset-overview">Dataset Overview&lt;/h2>
&lt;ul>
&lt;li>Video number: 5733 videos&lt;/li>
&lt;li>Video length: 300 frames/video (~1.7M total frames in the dataset)&lt;/li>
&lt;li>Challenge types: 12&lt;/li>
&lt;li>Challenge levels: 5
&lt;figure id="figure-challenging-conditions-horizontal-challenge-levels-vertical-challenge-types">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/curetsd/curetsd_challenges_hu05eb8e363895033479e945a3fafedd1b_1062714_2000x2000_fit_lanczos_2.png" data-caption="Challenging conditions (horizontal: challenge levels. vertical: challenge types).">
&lt;img data-src="https://minhungchen.netlify.app/project/curetsd/curetsd_challenges_hu05eb8e363895033479e945a3fafedd1b_1062714_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="1000">
&lt;/a>
&lt;figcaption>
Challenging conditions (horizontal: challenge levels. vertical: challenge types).
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;li>Traffic sign types: 14
&lt;figure id="figure-traffic-signs-row-1-real-signs-row-2-synthetic-signs">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/curetsd/sign_types_huff92360b86bedc6e357bd78014ffbff6_209212_2000x2000_fit_lanczos_2.png" data-caption="Traffic signs (Row 1: real signs. Row 2: synthetic signs).">
&lt;img data-src="https://minhungchen.netlify.app/project/curetsd/sign_types_huff92360b86bedc6e357bd78014ffbff6_209212_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="154">
&lt;/a>
&lt;figcaption>
Traffic signs (Row 1: real signs. Row 2: synthetic signs).
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;h3 id="demo-videos">Demo Videos&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Real data:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8V1LcpDlmjA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Synthetic data:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/bKnlJ_EWS8Q" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Please check our
&lt;a href="https://arxiv.org/pdf/1902.06857.pdf" target="_blank" rel="noopener">paper&lt;/a> for more results.&lt;/p>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;h4 id="papers--github">Papers &amp;amp; GitHub&lt;/h4>
&lt;p>
&lt;a href="https://github.com/olivesgatech/CURE-TSD" target="_blank" rel="noopener">
&lt;figure id="figure-githubhttpsgithubcomolivesgatechcure-tsd">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;figcaption>
&lt;a href="https://github.com/olivesgatech/CURE-TSD">&lt;strong>GitHub&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://arxiv.org/pdf/1908.11262.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-tits19httpsarxivorgpdf190811262pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="20%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1908.11262.pdf">TITS'19&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://arxiv.org/pdf/1902.06857.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-arxiv19httpsarxivorgpdf190206857pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="20%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1902.06857.pdf">arXiv'19&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;h4 id="download">Download&lt;/h4>
&lt;p>To download the dataset, please visit our
&lt;a href="https://github.com/olivesgatech/CURE-TSD" target="_blank" rel="noopener">GitHub&lt;/a> or
&lt;a href="https://ieee-dataport.org/open-access/cure-tsd-challenging-unreal-and-real-environment-traffic-sign-detection" target="_blank" rel="noopener">IEEE DataPort&lt;/a>.&lt;/p>
&lt;h4 id="other-links">Other Links&lt;/h4>
&lt;ul>
&lt;li>IEEE Xplore
[
&lt;a href="https://ieeexplore.ieee.org/document/8793235" target="_blank" rel="noopener">TITS'19&lt;/a> ]&lt;/li>
&lt;li>IEEE DataPort
[
&lt;a href="https://ieee-dataport.org/open-access/cure-tsd-challenging-unreal-and-real-environment-traffic-sign-detection" target="_blank" rel="noopener">CURE-TSD&lt;/a> ]&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="related-publications">Related Publications&lt;/h2>
&lt;p>If you find this project useful, please cite our papers (*equal contribution):&lt;/p>
&lt;ul>
&lt;li>Dogancan Temel,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, and Ghassan AlRegib, &amp;ldquo;Traffic Sign Detection under Challenging Conditions: A Deeper Look Into Performance Variations and Spectral Characteristics&amp;rdquo;,
&lt;a href="https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=6979" target="_blank" rel="noopener">&lt;em>IEEE Transactions on Intelligent Transportation Systems (TITS)&lt;/em>, 2019&lt;/a>.&lt;/li>
&lt;li>Dogancan Temel, Tariq Alshawi*,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen*&lt;/strong>&lt;/a>, and Ghassan AlRegib, &amp;ldquo;Challenging Environments for Traffic Sign Detection: Reliability Assessment under Inclement Conditions&amp;rdquo;, &lt;em>arXiv:1902.06857&lt;/em>, 2019.&lt;/li>
&lt;/ul>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@article{temel2019traffic,
title={Traffic sign detection under challenging conditions: A deeper look into performance variations and spectral characteristics},
author={Temel, Dogancan and Chen, Min-Hung and AlRegib, Ghassan},
journal={IEEE Transactions on Intelligent Transportation Systems (TITS)},
year={2019},
publisher={IEEE}
}
@article{temel2019challenging,
title={Challenging environments for traffic sign detection: Reliability assessment under inclement conditions},
author={Temel, Dogancan and Alshawi, Tariq and Chen, Min-Hung and AlRegib, Ghassan},
journal={arXiv preprint arXiv:1902.06857},
year={2019},
url={https://arxiv.org/abs/1902.06857}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>Georgia Institute of Technology &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="http://cantemel.com/" target="_blank" rel="noopener">
&lt;figure id="figure-dogancan-temel">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ct.jpg" alt="" width="100%" >
&lt;figcaption>
Dogancan Temel
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/tariq-alshawi/" target="_blank" rel="noopener">
&lt;figure id="figure-tariq-alshawi">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ta.jpg" alt="" width="100%" >
&lt;figcaption>
Tariq Alshawi
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregib">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</title><link>https://minhungchen.netlify.app/publication/ta3n/</link><pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/ta3n/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="videos">Videos&lt;/h2>
&lt;p>Oral Video (please turn on closed captions):
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/j9cDuzmpYP8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;figure id="figure-codehttpsgithubcomcmhungsteveta3n">
&lt;a href="https://github.com/cmhungsteve/TA3N">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/cmhungsteve/TA3N">&lt;strong>Code&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-paperhttpsarxivorgpdf190712743pdf">
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf">Paper&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-slideshttpswwwdropboxcomss9ud77a1zt0vqbnoral_ta3n_iccv_2019_mutepdfdl0">
&lt;a href="https://www.dropbox.com/s/s9ud77a1zt0vqbn/Oral_TA3N_ICCV_2019_mute.pdf?dl=0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/s9ud77a1zt0vqbn/Oral_TA3N_ICCV_2019_mute.pdf?dl=0">Slides&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-posterhttpswwwdropboxcomsg16v02k9sgn1xibiccv2019_steve_ta3n_poster_v1_2pdfdl0">
&lt;a href="https://www.dropbox.com/s/g16v02k9sgn1xib/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/g16v02k9sgn1xib/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0">Poster&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.html" target="_blank" rel="noopener">CVF Open Access&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://ieeexplore.ieee.org/document/9008391" target="_blank" rel="noopener">IEEE Xplore&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://mlatgt.blog/2019/09/10/overcoming-large-scale-annotation-requirements-for-understanding-videos-in-the-wild" target="_blank" rel="noopener">ML@GT Blog&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://mailchi.mp/4ed0cbf6a67d/iccv2019" target="_blank" rel="noopener">GT@ICCV'19&lt;/a>&lt;/li>
&lt;li>CVPR workshop on Learning from Unlabeled Videos (LUV) [
&lt;a href="https://sites.google.com/view/luv2019/program" target="_blank" rel="noopener">CVPRW'19&lt;/a> ]&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;ul>
&lt;li>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">Zsolt Kira&lt;/a>,
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>,
&lt;a href="https://www.linkedin.com/in/jaekwon-yoo-8685862b/" target="_blank" rel="noopener">Jaekwon Yoo&lt;/a>,
&lt;a href="%28https://www.linkedin.com/in/ruxin-chen-991477119/%29">Ruxin Chen&lt;/a>, and
&lt;a href="https://scholar.google.com/citations?user=5YR6dTEAAAAJ" target="_blank" rel="noopener">Jian Zheng&lt;/a>, &amp;ldquo;Temporal Attentive Alignment for Large-Scale Video Domain Adaptation&amp;rdquo;,
&lt;a href="http://iccv2019.thecvf.com/" target="_blank" rel="noopener">&lt;em>IEEE International Conference on Computer Vision (ICCV)&lt;/em>, 2019&lt;/a> &lt;strong>[Oral (acceptance rate: 4.6%), travel grant awarded]&lt;/strong>.&lt;/li>
&lt;li>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">Zsolt Kira&lt;/a>, and
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>, &amp;ldquo;Temporal Attentive Alignment for Video Domain Adaptation&amp;rdquo;,
&lt;a href="https://sites.google.com/view/luv2019" target="_blank" rel="noopener">&lt;em>CVPR Workshop on Learning from Unlabeled Videos (LUV)&lt;/em>, 2019&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{chen2019temporal,
title={Temporal attentive alignment for large-scale video domain adaptation},
author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan and Yoo, Jaekwon and Chen, Ruxin and Zheng, Jian},
booktitle={IEEE International Conference on Computer Vision (ICCV)},
year={2019}
}
@article{chen2019taaan,
title={Temporal Attentive Alignment for Video Domain Adaptation},
author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan},
journal={CVPR Workshop on Learning from Unlabeled Videos},
year={2019},
url={https://arxiv.org/abs/1905.10861}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>Georgia Institute of Technology   &lt;sup>2&lt;/sup>Sony Interactive Entertainment LLC   &lt;sup>3&lt;/sup>Binghamton University &lt;br>&lt;/strong>
*work partially done as a SIE intern&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">
&lt;figure id="figure-zsolt-kirasup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_zk.jpg" alt="" width="100%" >
&lt;figcaption>
Zsolt Kira&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregibsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/jaekwon-yoo-8685862b/" target="_blank" rel="noopener">
&lt;figure id="figure-jaekwon-yoosup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Jaekwon Yoo&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/ruxin-chen-991477119/" target="_blank" rel="noopener">
&lt;figure id="figure-ruxin-chensup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Ruxin Chen&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=5YR6dTEAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-jian-zhengsup3sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_jz.jpg" alt="" width="100%" >
&lt;figcaption>
Jian Zheng&lt;sup>3&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Traffic Sign Detection Under Challenging Conditions: A Deeper Look into Performance Variations and Spectral Characteristics</title><link>https://minhungchen.netlify.app/publication/curetsd/</link><pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/curetsd/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="demo-videos">Demo Videos&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Real data:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8V1LcpDlmjA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Synthetic data:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/bKnlJ_EWS8Q" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-githubhttpsgithubcomolivesgatechcure-tsd">
&lt;a href="https://github.com/olivesgatech/CURE-TSD">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="65%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/olivesgatech/CURE-TSD">&lt;strong>GitHub&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;a href="https://arxiv.org/pdf/1908.11262.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-paper-arxivhttpsarxivorgpdf190811262pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="15%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1908.11262.pdf">Paper (arXiv)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://ieeexplore.ieee.org/document/8793235" target="_blank" rel="noopener">IEEE Xplore&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="http://cantemel.com/" target="_blank" rel="noopener">Dogancan Temel&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, and
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>, &amp;ldquo;Traffic Sign Detection under Challenging Conditions: A Deeper Look Into Performance Variations and Spectral Characteristics&amp;rdquo;,
&lt;a href="https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=6979" target="_blank" rel="noopener">&lt;em>IEEE Transactions on Intelligent Transportation Systems (TITS)&lt;/em>, 2019&lt;/a>.&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@article{temel2019traffic,
title={Traffic sign detection under challenging conditions: A deeper look into performance variations and spectral characteristics},
author={Temel, Dogancan and Chen, Min-Hung and AlRegib, Ghassan},
journal={IEEE Transactions on Intelligent Transportation Systems (TITS)},
year={2019},
publisher={IEEE}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>Georgia Institute of Technology &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="http://cantemel.com/" target="_blank" rel="noopener">
&lt;figure id="figure-dogancan-temel">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ct.jpg" alt="" width="100%" >
&lt;figcaption>
Dogancan Temel
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregib">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item></channel></rss>