<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SPIC | Min-Hung Chen</title><link>https://minhungchen.netlify.app/tag/spic/</link><atom:link href="https://minhungchen.netlify.app/tag/spic/index.xml" rel="self" type="application/rss+xml"/><description>SPIC</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Min-Hung Chen</copyright><lastBuildDate>Mon, 03 May 2021 14:00:00 +0000</lastBuildDate><image><url>https://minhungchen.netlify.app/img/authors/head_me.jpg</url><title>SPIC</title><link>https://minhungchen.netlify.app/tag/spic/</link></image><item><title>Bridging Distributional Discrepancy with Temporal Dynamics for Video Understanding</title><link>https://minhungchen.netlify.app/talk/phd_as21/</link><pubDate>Mon, 03 May 2021 14:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/talk/phd_as21/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click on the &lt;strong>Slides&lt;/strong> button above to view the built-in slides feature.
&lt;/div>
&lt;/div>
Slides can be added in a few ways:
- **Create** slides using Academic's [*Slides*](https://sourcethemes.com/academic/docs/managing-content/#create-slides) feature and link using `slides` parameter in the front matter of the talk file
- **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file
- **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://sourcethemes.com/academic/docs/writing-markdown-latex/).
Further talk details can easily be added to this page using *Markdown* and $\rm \LaTeX$ math code. -->
&lt;p>Feel free to click the upper button &lt;code>Slides&lt;/code> to check the full talk slides. &lt;br>
Note: This talk was invited by
&lt;a href="https://www.citi.sinica.edu.tw/pages/pullpull/index_en.html" target="_blank" rel="noopener">Dr. Jun-Cheng Chen&lt;/a>.&lt;/p></description></item><item><title>My Research Journey for Video Understanding</title><link>https://minhungchen.netlify.app/talk/video_nctu21/</link><pubDate>Thu, 07 Jan 2021 10:10:00 +0000</pubDate><guid>https://minhungchen.netlify.app/talk/video_nctu21/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click on the &lt;strong>Slides&lt;/strong> button above to view the built-in slides feature.
&lt;/div>
&lt;/div>
Slides can be added in a few ways:
- **Create** slides using Academic's [*Slides*](https://sourcethemes.com/academic/docs/managing-content/#create-slides) feature and link using `slides` parameter in the front matter of the talk file
- **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file
- **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://sourcethemes.com/academic/docs/writing-markdown-latex/).
Further talk details can easily be added to this page using *Markdown* and $\rm \LaTeX$ math code. -->
&lt;p>Feel free to click the upper button &lt;code>Slides&lt;/code> to check the full talk slides. &lt;br>
Note:
&lt;a href="https://sites.google.com/site/yylinweb/" target="_blank" rel="noopener">Prof. Yen-Yu Lin&lt;/a> was my supervisor when I was a Research Assistant at
&lt;a href="https://www.citi.sinica.edu.tw/en/" target="_blank" rel="noopener">Academia Sinica&lt;/a> during 2013-14.&lt;/p></description></item><item><title>Bridging Distributional Discrepancy with Temporal Dynamics for Video Understanding</title><link>https://minhungchen.netlify.app/publication/phd/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/phd/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="resources">Resources&lt;/h2>
&lt;figure id="figure-dissertationhttpssmartechgatecheduhandle185363572">
&lt;a href="https://smartech.gatech.edu/handle/1853/63572">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="15%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://smartech.gatech.edu/handle/1853/63572">&lt;strong>Dissertation&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-code-ts-lstmhttpsgithubcomchihyaomaactivity-recognition-with-cnn-and-rnn">
&lt;a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="80%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN">Code (TS-LSTM)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-code-ta3nhttpsgithubcomcmhungsteveta3n">
&lt;a href="https://github.com/cmhungsteve/TA3N">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="80%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/cmhungsteve/TA3N">Code (TA3N)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-code-sstdahttpsgithubcomcmhungstevesstda">
&lt;a href="https://github.com/cmhungsteve/SSTDA">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="80%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/cmhungsteve/SSTDA">Code (SSTDA)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, &amp;ldquo;Bridging Distributional Discrepancy with Temporal Dynamics for Video Understanding&amp;rdquo;, &lt;em>PhD Dissertation, Georgia Institute of Technology&lt;/em>, 2020.&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@phdthesis{chen2020bridging,
title={Bridging Distributional Discrepancy with Temporal Dynamics for Video Understanding},
author={Chen, Min-Hung},
year={2020},
school={Georgia Institute of Technology}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>Georgia Institute of Technology&lt;/strong>&lt;/p>
&lt;p>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="20%" >
&lt;figcaption>
Min-Hung Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/p></description></item><item><title>Activity Recognition with RNN and Temporal-ConvNet</title><link>https://minhungchen.netlify.app/project/tslstm/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/project/tslstm/</guid><description/></item><item><title>TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition</title><link>https://minhungchen.netlify.app/publication/tslstm/</link><pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/tslstm/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="demo-videos">Demo Videos&lt;/h2>
&lt;p>Please check our
&lt;a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN" target="_blank" rel="noopener">GitHub&lt;/a>.&lt;/p>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;figure id="figure-codehttpsgithubcomchihyaomaactivity-recognition-with-cnn-and-rnn">
&lt;a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN">&lt;strong>Code&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-paper-arxivhttpsarxivorgabs170310667">
&lt;a href="https://arxiv.org/abs/1703.10667">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="25%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://arxiv.org/abs/1703.10667">Paper (arXiv)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-poster-mlgthttpswwwdropboxcoms1c7fsgr3ef1x9v6poster_tslstm-teminception_20170417pdfdl0">
&lt;a href="https://www.dropbox.com/s/1c7fsgr3ef1x9v6/poster_TSLSTM-TemInception_20170417.pdf?dl=0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="25%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/1c7fsgr3ef1x9v6/poster_TSLSTM-TemInception_20170417.pdf?dl=0">Poster (ML@GT)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0923596518304922" target="_blank" rel="noopener">SPIC Journal&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="https://chihyaoma.github.io/" target="_blank" rel="noopener">Chih-Yao Ma*&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen*&lt;/strong>&lt;/a>,
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">Zsolt Kira&lt;/a>, and
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>, &amp;ldquo;TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition&amp;rdquo;,
&lt;a href="https://www.sciencedirect.com/journal/signal-processing-image-communication/vol/71/" target="_blank" rel="noopener">&lt;em>Signal Processing: Image Communication (SPIC)&lt;/em>, 2019&lt;/a>. (*equal contribution)&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@article{ma2019ts,
title={TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition},
author={Ma, Chih-Yao and Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan},
journal={Signal Processing: Image Communication},
volume={71},
pages={76--87},
year={2019},
publisher={Elsevier}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>Georgia Institute of Technology &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://chihyaoma.github.io/" target="_blank" rel="noopener">
&lt;figure id="figure-chih-yao-ma">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_cym.jpg" alt="" width="100%" >
&lt;figcaption>
Chih-Yao Ma
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">
&lt;figure id="figure-zsolt-kira">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_zk.jpg" alt="" width="100%" >
&lt;figcaption>
Zsolt Kira
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregib">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item></channel></rss>