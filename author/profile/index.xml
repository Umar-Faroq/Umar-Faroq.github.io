<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Min-Hung Chen</title><link>https://minhungchen.netlify.app/author/min-hung-chen/</link><atom:link href="https://minhungchen.netlify.app/author/min-hung-chen/index.xml" rel="self" type="application/rss+xml"/><description>Min-Hung Chen</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Min-Hung Chen</copyright><image><url>https://minhungchen.netlify.app/author/min-hung-chen/avatar_huc3acf39b4c2265ec309e042ecb270b7c_902720_270x270_fill_lanczos_center_2.png</url><title>Min-Hung Chen</title><link>https://minhungchen.netlify.app/author/min-hung-chen/</link></image><item><title>Learned Smartphone ISP on Mobile NPUs With Deep Learning, Mobile AI 2021 Challenge: Report</title><link>https://minhungchen.netlify.app/publication/mai/</link><pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/mai/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="resources">Resources&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-paper-arxivhttpsarxivorgpdf210507809pdf">
&lt;a href="https://arxiv.org/pdf/2105.07809.pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="15%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/2105.07809.pdf">Paper (arXiv)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Ignatov_Learned_Smartphone_ISP_on_Mobile_NPUs_With_Deep_Learning_Mobile_CVPRW_2021_paper.html" target="_blank" rel="noopener">CVF Open Access&lt;/a>&lt;/li>
&lt;li>
&lt;a href="">IEEE Xplore (to be updated)&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://ai-benchmark.com/workshops/mai/2021/" target="_blank" rel="noopener">Mobile AI (MAI) Workshop @ CVPR 2021&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://competitions.codalab.org/competitions/28054" target="_blank" rel="noopener">Learned Smartphone ISP Challenge @ MAI 2021&lt;/a>&lt;/li>
&lt;li>MediaTek Blog [
&lt;a href="https://www.mediatek.com/blog/mediateks-summer-of-ai-at-cvpr-2021" target="_blank" rel="noopener">Blog-1&lt;/a> ][
&lt;a href="https://www.mediatek.com/blog/mediatek-hosts-cvpr-and-icmr-challenges-sign-up-today" target="_blank" rel="noopener">Blog-2&lt;/a> ]&lt;/li>
&lt;li>
&lt;a href="https://www.mediatek.com/products/smartphones/dimensity-1000-series" target="_blank" rel="noopener">MediaTek Dimensity 1000+ APU&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;ul>
&lt;li>
&lt;a href="https://scholar.google.com/citations?user=kBoWvhIAAAAJ" target="_blank" rel="noopener">Andrey Ignatov&lt;/a>,
&lt;a href="https://scholar.google.com/citations?user=CyMTGlsAAAAJ" target="_blank" rel="noopener">Cheng-Ming Chiang&lt;/a>,
&lt;a href="https://scholar.google.com/citations?user=EeVsWaUAAAAJ" target="_blank" rel="noopener">Hsien-Kai Kuo&lt;/a>, Anastasia Sycheva,
&lt;a href="https://people.ee.ethz.ch/~timofter/" target="_blank" rel="noopener">Radu Timofte&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://www.linkedin.com/in/man-yu-lee-b207471aa/" target="_blank" rel="noopener">Man-Yu Lee&lt;/a>,
&lt;a href="https://xusean0118.github.io/" target="_blank" rel="noopener">Yu-Syuan Xu&lt;/a>, Yu Tseng, et al. &amp;ldquo;Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021 Challenge: Report&amp;rdquo;,
&lt;a href="https://ai-benchmark.com/workshops/mai/2021/" target="_blank" rel="noopener">&lt;em>CVPR Workshop and Challenges on Mobile AI (MAI)&lt;/em>, 2021&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{ignatov2021learned,
title={Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021 Challenge: Report},
author={Ignatov, Andrey and Chiang, Cheng-Ming and Kuo, Hsien-Kai and Sycheva, Anastasia and Timofte, Radu and Chen, Min-Hung and Lee, Man-Yu and Xu, Yu-Syuan and Tseng, Yu and Xu, Shusong and Guo, Jin and Chen, Chao-Hung and Hsyu, Ming-Chun and Tsai, Wen-Chia and Chen, Chao-Wei and Malivenko, Grigory and Kwon, Minsu and Lee, Myungje and Yoo, Jaeyoon and Kang, Changbeom and Wang, Shinjo and Shaolong, Zheng and Dejun, Hao and Fen, Xie and Zhuang, Feng and Ma, Yipeng and Peng, Jingyang and Wang, Tao and Song, Fenglong and Hsu, Chih-Chung and Chen, Kwan-Lin and Wu, Mei-Hsuang and Chudasama, Vishal and Prajapati, Kalpesh and Patel, Heena and Sarvaiya, Anjali and Upla, Kishor and Raja, Kiran and Ramachandra, Raghavendra and Busch, Christoph and de Stoutz, Etienne},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops (Mobile AI)},
year={2021},
url={https://arxiv.org/abs/2105.07809}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>MediaTek Inc.   &lt;sup>2&lt;/sup>ETH Zurich &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=kBoWvhIAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-andrey-ignatovsup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ai.jpg" alt="" width="100%" >
&lt;figcaption>
Andrey Ignatov&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=CyMTGlsAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-cheng-ming-chiangsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_cmc.jpg" alt="" width="100%" >
&lt;figcaption>
Cheng-Ming Chiang&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=EeVsWaUAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-hsien-kai-kuosup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_hkk.jpg" alt="" width="100%" >
&lt;figcaption>
Hsien-Kai Kuo&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="">
&lt;figure id="figure-anastasia-sychevasup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Anastasia Sycheva&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://people.ee.ethz.ch/~timofter/" target="_blank" rel="noopener">
&lt;figure id="figure-radu-timoftesup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_rt.jpg" alt="" width="100%" >
&lt;figcaption>
Radu Timofte&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/man-yu-lee-b207471aa/" target="_blank" rel="noopener">
&lt;figure id="figure-man-yu-leesup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_myl.jpg" alt="" width="100%" >
&lt;figcaption>
Man-Yu Lee&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://xusean0118.github.io/" target="_blank" rel="noopener">
&lt;figure id="figure-yu-syuan-xusup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ysx.jpg" alt="" width="100%" >
&lt;figcaption>
Yu-Syuan Xu&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="">
&lt;figure id="figure-yu-tsengsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Yu Tseng&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Network Space Search for Pareto-Efficient Spaces</title><link>https://minhungchen.netlify.app/publication/nss/</link><pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/nss/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="video">Video&lt;/h2>
&lt;h2 id="hahahugoshortcode-s2-hbhb">2-min talk video:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/N1ezhkxGTN8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/h2>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-paper-arxivhttpsarxivorgpdf210411014pdf">
&lt;a href="https://arxiv.org/pdf/2104.11014.pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="15%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/2104.11014.pdf">Paper (arXiv)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Hong_Network_Space_Search_for_Pareto-Efficient_Spaces_CVPRW_2021_paper.html" target="_blank" rel="noopener">CVF Open Access&lt;/a>&lt;/li>
&lt;li>
&lt;a href="">IEEE Xplore (to be updated)&lt;/a>&lt;/li>
&lt;li>CVPR workshop on Efficient Deep Learning for Computer Vision (ECV) [
&lt;a href="https://sites.google.com/view/ecv2021" target="_blank" rel="noopener">CVPRW'21&lt;/a> ]&lt;/li>
&lt;li>
&lt;a href="https://www.mediatek.com/blog/mediateks-summer-of-ai-at-cvpr-2021" target="_blank" rel="noopener">MediaTek Blog&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="https://scholar.google.com/citations?user=QRZuu3AAAAAJ" target="_blank" rel="noopener">Min-Fong Hong&lt;/a>,
&lt;a href="https://scholar.google.com/citations?user=zbhpHsYAAAAJ" target="_blank" rel="noopener">Hao-Yun Chen&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://xusean0118.github.io/" target="_blank" rel="noopener">Yu-Syuan Xu&lt;/a>,
&lt;a href="https://scholar.google.com/citations?user=EeVsWaUAAAAJ" target="_blank" rel="noopener">Hsien-Kai Kuo&lt;/a>,
&lt;a href="https://scholar.google.com.tw/citations?user=XomuoxsAAAAJ" target="_blank" rel="noopener">Yi-Min Tsai&lt;/a>,
&lt;a href="https://www.linkedin.com/in/hung-jen-hj-chen-a3620ab4/" target="_blank" rel="noopener">Hung-Jen Chen&lt;/a>, and Kevin Jou, &amp;ldquo;Network Space Search for Pareto-Efficient Spaces
&amp;ldquo;,
&lt;a href="https://sites.google.com/view/ecv2021" target="_blank" rel="noopener">&lt;em>CVPR Workshop on Efficient Deep Learning for Computer Vision (CVPRW)&lt;/em>, 2021&lt;/a> &lt;strong>[Oral]&lt;/strong>.&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{hong2021network,
title={Network Space Search for Pareto-Efficient Spaces},
author={Hong, Min-Fong and Chen, Hao-Yun and Chen, Min-Hung and Xu, Yu-Syuan and Kuo, Hsien-Kai and Tsai, Yi-Min and Chen, Hung-Jen and Jou, Kevin},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops (Efficient Deep Learning for Computer Vision)},
year={2021},
url={https://arxiv.org/abs/2104.11014}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>MediaTek Inc.&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=QRZuu3AAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-min-fong-hong">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_mfh.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Fong Hong
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=zbhpHsYAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-hao-yun-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_hyc.jpg" alt="" width="100%" >
&lt;figcaption>
Hao-Yun Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://xusean0118.github.io/" target="_blank" rel="noopener">
&lt;figure id="figure-yu-syuan-xu">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ysx.jpg" alt="" width="100%" >
&lt;figcaption>
Yu-Syuan Xu
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=EeVsWaUAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-hsien-kai-kuo">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_hkk.jpg" alt="" width="100%" >
&lt;figcaption>
Hsien-Kai Kuo
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com.tw/citations?user=XomuoxsAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-yi-min-tsai">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ymt.jpg" alt="" width="100%" >
&lt;figcaption>
Yi-Min Tsai
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/hung-jen-hj-chen-a3620ab4/" target="_blank" rel="noopener">
&lt;figure id="figure-hung-jen-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_hjc.jpg" alt="" width="100%" >
&lt;figcaption>
Hung-Jen Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="">
&lt;figure id="figure-kevin-jou">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_kj.jpg" alt="" width="100%" >
&lt;figcaption>
Kevin Jou
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Bridging Distributional Discrepancy with Temporal Dynamics for Video Understanding</title><link>https://minhungchen.netlify.app/publication/phd/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/phd/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="resources">Resources&lt;/h2>
&lt;figure id="figure-dissertationhttpssmartechgatecheduhandle185363572">
&lt;a href="https://smartech.gatech.edu/handle/1853/63572">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="15%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://smartech.gatech.edu/handle/1853/63572">&lt;strong>Dissertation&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-code-ts-lstmhttpsgithubcomchihyaomaactivity-recognition-with-cnn-and-rnn">
&lt;a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="80%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN">Code (TS-LSTM)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-code-ta3nhttpsgithubcomcmhungsteveta3n">
&lt;a href="https://github.com/cmhungsteve/TA3N">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="80%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/cmhungsteve/TA3N">Code (TA3N)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-code-sstdahttpsgithubcomcmhungstevesstda">
&lt;a href="https://github.com/cmhungsteve/SSTDA">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="80%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/cmhungsteve/SSTDA">Code (SSTDA)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, &amp;ldquo;Bridging Distributional Discrepancy with Temporal Dynamics for Video Understanding&amp;rdquo;, &lt;em>PhD Dissertation, Georgia Institute of Technology&lt;/em>, 2020.&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@phdthesis{chen2020bridging,
title={Bridging Distributional Discrepancy with Temporal Dynamics for Video Understanding},
author={Chen, Min-Hung},
year={2020},
school={Georgia Institute of Technology}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>Georgia Institute of Technology&lt;/strong>&lt;/p>
&lt;p>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="20%" >
&lt;figcaption>
Min-Hung Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/p></description></item><item><title>Action Segmentation with Temporal Domain Adaptation</title><link>https://minhungchen.netlify.app/project/cdas/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/project/cdas/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Despite the recent progress of fully-supervised action segmentation techniques, the performance is still not fully satisfactory.
Exploiting larger-scale labeled data and designing more complicated architectures result in additional annotation and computation costs.
Therefore, we aim to exploit auxiliary unlabeled videos, which are
comparatively easy to obtain, to improve the performance.&lt;/p>
&lt;hr>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;p>One main challenge of utilizing unlabeled videos is the problem of &lt;strong>spatio-temporal variations&lt;/strong>. For example, different people may &lt;em>make tea&lt;/em> in different personalized styles even if the given recipe is the same. The intra-class variations cause degraded performance by directly deploying a model trained with different groups of people.&lt;/p>
&lt;hr>
&lt;h2 id="our-approaches">Our Approaches&lt;/h2>
&lt;p>We exploit unlabeled videos to address this problem by reformulating the action segmentation task as a cross-domain problem with domain discrepancy caused by spatio-temporal variations. &lt;br>
To reduce the discrepancy, we propose two approaches:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mixed Temporal Domain Adaptation (MTDA)&lt;/strong>: align the features embedded with local and global temporal dynamics.
&lt;figure >
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdas/approach_MTDA_hucffefdbeb1823fdb487e1b3e7c55d0f8_535391_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="https://minhungchen.netlify.app/project/cdas/approach_MTDA_hucffefdbeb1823fdb487e1b3e7c55d0f8_535391_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="70%" height="739">
&lt;/a>
&lt;/figure>
&lt;/li>
&lt;li>&lt;strong>Self-Supervised Temporal Domain Adaptation (SSTDA)&lt;/strong>: align the feature spaces across multiple temporal scales with self-supervised learning.
&lt;figure >
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdas/approach_SSTDA_hu576d5af5b9c05786c04595278edfc0cf_152205_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="https://minhungchen.netlify.app/project/cdas/approach_SSTDA_hu576d5af5b9c05786c04595278edfc0cf_152205_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="100%" height="624">
&lt;/a>
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>We evaluate three challenging benchmark datasets: &lt;strong>GTEA&lt;/strong>, &lt;strong>50Salads&lt;/strong>, and &lt;strong>Breakfast&lt;/strong>, and achieve the follows:&lt;/p>
&lt;ul>
&lt;li>Outperform other Domain Adaptation (DA) and video-based self-supervised approaches.
&lt;figure id="figure-the-comparison-of-different-methods-that-can-learn-information-from-unlabeled-target-videos-on-gtea">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdas/result_SSTDA_DA_hu461714772ec65b7d7fa817e5e0e7d2dd_84503_2000x2000_fit_lanczos_2.png" data-caption="The comparison of different methods that can learn information from unlabeled target videos (on GTEA).">
&lt;img data-src="https://minhungchen.netlify.app/project/cdas/result_SSTDA_DA_hu461714772ec65b7d7fa817e5e0e7d2dd_84503_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="50%" height="361">
&lt;/a>
&lt;figcaption>
The comparison of different methods that can learn information from unlabeled target videos (on GTEA).
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;li>Outperform the current state-of-the-art action segmentation methods by large margins.&lt;/li>
&lt;li>Achieve comparable performance with fully-supervised methods using only 65% of the labeled training data.
&lt;figure id="figure-comparison-with-the-most-recent-action-segmentation-methods-on-all-three-datasets">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdas/result_SSTDA_SOTA_hu47e5dce5026a3482fb93f68f57e4d656_162875_2000x2000_fit_lanczos_2.png" data-caption="Comparison with the most recent action segmentation methods on all three datasets.">
&lt;img data-src="https://minhungchen.netlify.app/project/cdas/result_SSTDA_SOTA_hu47e5dce5026a3482fb93f68f57e4d656_162875_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="50%" height="639">
&lt;/a>
&lt;figcaption>
Comparison with the most recent action segmentation methods on all three datasets.
&lt;/figcaption>
&lt;/figure>
&lt;figure id="figure--the-visualization-of-temporal-action-segmentation-for-our-methods-with-color-coding-input-example-make-coffee">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdas/result_SSTDA_visualization_hude82285d87074eb140b18b35b3cd7515_99399_2000x2000_fit_lanczos_2.png" data-caption="The visualization of temporal action segmentation for our methods with color-coding (input example: make coffee)">
&lt;img data-src="https://minhungchen.netlify.app/project/cdas/result_SSTDA_visualization_hude82285d87074eb140b18b35b3cd7515_99399_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="100%" height="310">
&lt;/a>
&lt;figcaption>
The visualization of temporal action segmentation for our methods with color-coding (input example: make coffee)
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;p>Please check our
&lt;a href="https://arxiv.org/pdf/2003.02824.pdf" target="_blank" rel="noopener">papers&lt;/a> for more results.&lt;/p>
&lt;hr>
&lt;h2 id="videos">Videos&lt;/h2>
&lt;p>Overview Introduction:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/HxPYhOZco-4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>CVPR'20 Presentation (1-min):
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/sKCXZksFOWA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>CVPR'20 Presentation (5-min):
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/16OTdVeKahs" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>WACV'20 Presentation:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/RkdhHDvj4UA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;h4 id="papers--code">Papers &amp;amp; Code&lt;/h4>
&lt;p>
&lt;a href="https://github.com/cmhungsteve/SSTDA" target="_blank" rel="noopener">
&lt;figure id="figure-codehttpsgithubcomcmhungstevesstda">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;figcaption>
&lt;a href="https://github.com/cmhungsteve/SSTDA">&lt;strong>Code&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://arxiv.org/pdf/2003.02824.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-cvpr20httpsarxivorgpdf200302824pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="20%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/2003.02824.pdf">CVPR'20&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-wacv20httpopenaccessthecvfcomcontent_wacv_2020paperschen_action_segmentation_with_mixed_temporal_domain_adaptation_wacv_2020_paperpdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="20%" >
&lt;figcaption>
&lt;a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.pdf">WACV'20&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;h4 id="presentations">Presentations&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://www.dropbox.com/s/4cwnwxsqp98mnwf/Oral_SSTDA_CVPR_2020_mute.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-slides-cvpr20">
&lt;img data-src="https://minhungchen.netlify.app/project/cdas/slides_CVPR2020_5min_hua8f37129a31f67c12b56cbe76af8ec8b_289280_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="720">
&lt;figcaption>
Slides (CVPR'20)
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.dropbox.com/s/bvkt581lk049zrl/Oral_MTDA_WACV_2020_mute.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-slides-wacv20">
&lt;img data-src="https://minhungchen.netlify.app/project/cdas/slides_WACV2020_hu90267c901349e116f78bd18d30269522_68620_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="720">
&lt;figcaption>
Slides (WACV'20)
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://www.dropbox.com/s/83sj8yf9b9a9qtq/CVPR2020_Steve_SSTDA_poster_v1.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-poster-cvpr20">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdas/poster_CVPR2020_hucd8598a49fd7bde93d2d4b609b38c663_3490067_2000x2000_fit_lanczos_2.png" data-caption="Poster (CVPR&amp;#39;20)">
&lt;img data-src="https://minhungchen.netlify.app/project/cdas/poster_CVPR2020_hucd8598a49fd7bde93d2d4b609b38c663_3490067_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="2256">
&lt;/a>
&lt;figcaption>
Poster (CVPR'20)
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.dropbox.com/s/yzj25ok1ojb4qwn/WACV2020_Steve_MTDA_poster_v1.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-poster-wacv20">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdas/poster_WACV2020_hu9313cdbd3861dca886f5c015f7c05644_2452412_2000x2000_fit_lanczos_2.png" data-caption="Poster (WACV&amp;#39;20)">
&lt;img data-src="https://minhungchen.netlify.app/project/cdas/poster_WACV2020_hu9313cdbd3861dca886f5c015f7c05644_2452412_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="2256">
&lt;/a>
&lt;figcaption>
Poster (WACV'20)
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;h4 id="other-links">Other Links&lt;/h4>
&lt;ul>
&lt;li>CVF Open Access
[
&lt;a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Action_Segmentation_With_Joint_Self-Supervised_Temporal_Domain_Adaptation_CVPR_2020_paper.html" target="_blank" rel="noopener">CVPR'20&lt;/a> ]
[
&lt;a href="http://openaccess.thecvf.com/content_WACV_2020/html/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.html" target="_blank" rel="noopener">WACV'20&lt;/a> ]&lt;/li>
&lt;li>IEEE Xplore
[
&lt;a href="https://ieeexplore.ieee.org/document/9157452" target="_blank" rel="noopener">CVPR'20&lt;/a> ]
[
&lt;a href="https://ieeexplore.ieee.org/document/9093535" target="_blank" rel="noopener">WACV'20&lt;/a> ]&lt;/li>
&lt;li>ML@GT [
&lt;a href="https://mlgt-at-cvpr-2020.mailchimpsites.com/" target="_blank" rel="noopener">Article&lt;/a> ]&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="related-publications">Related Publications&lt;/h2>
&lt;p>If you find this project useful, please cite our papers:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, Baopu Li, Yingze Bao, Ghassan AlRegib, and Zsolt Kira, &amp;ldquo;Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation&amp;rdquo;,
&lt;a href="http://cvpr2020.thecvf.com/" target="_blank" rel="noopener">&lt;em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em>, 2020&lt;/a>.&lt;/li>
&lt;li>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, Baopu Li, Yingze Bao, and Ghassan AlRegib, &amp;ldquo;Action Segmentation with Mixed Temporal Domain Adaptation&amp;rdquo;,
&lt;a href="http://wacv20.wacv.net/" target="_blank" rel="noopener">&lt;em>IEEE Winter Conference on Applications of Computer Vision (WACV)&lt;/em>, 2020&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{chen2020action,
title={Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation},
author={Chen, Min-Hung and Li, Baopu and Bao, Yingze and AlRegib, Ghassan and Kira, Zsolt},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2020}
}
@inproceedings{chen2020mixed,
title={Action Segmentation with Mixed Temporal Domain Adaptation},
author={Chen, Min-Hung and Li, Baopu and Bao, Yingze and AlRegib, Ghassan},
booktitle={IEEE Winter Conference on Applications of Computer Vision (WACV)},
year={2020}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>Georgia Institute of Technology   &lt;sup>2&lt;/sup>Baidu USA &lt;br>&lt;/strong>
*work done during an internship at Baidu USA&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://dblp.org/pers/l/Li:Baopu.html" target="_blank" rel="noopener">
&lt;figure id="figure-baopu-lisup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_bl.jpg" alt="" width="100%" >
&lt;figcaption>
Baopu Li&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?hl=en&amp;amp;user=WiAQC68AAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate" target="_blank" rel="noopener">
&lt;figure id="figure-yingze-baosup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_yb.jpg" alt="" width="100%" >
&lt;figcaption>
Yingze Bao&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregibsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">
&lt;figure id="figure-zsolt-kirasup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_zk.jpg" alt="" width="100%" >
&lt;figcaption>
Zsolt Kira&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Temporal Attentive Alignment for Video Domain Adaptation</title><link>https://minhungchen.netlify.app/project/cdar/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/project/cdar/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Furthermore, there do not exist well-organized datasets to evaluate and benchmark the performance of DA algorithms for videos. Therefore, new datasets and approaches for video DA are desired.&lt;/p>
&lt;hr>
&lt;h2 id="challenges">Challenges&lt;/h2>
&lt;p>Videos can suffer from domain shift along both the spatial and temporal directions, bringing the need of alignment for embedded feature spaces along both directions. However, most DA approaches focus on spatial direction only.&lt;/p>
&lt;hr>
&lt;h2 id="our-approaches">Our Approaches&lt;/h2>
&lt;p>We propose &lt;strong>Temporal Attentive Adversarial Adaptation Network (TA&lt;sup>3&lt;/sup>N)&lt;/strong> to simultaneously attend, align and learn temporal dynamics across domains. TA&lt;sup>3&lt;/sup>N contains the following components:&lt;/p>
&lt;ul>
&lt;li>&lt;em>TemRelation module&lt;/em>: learn various local temporal features embedded with relation information in different temporal scales&lt;/li>
&lt;li>&lt;em>Domain attention mechanism&lt;/em>: align local features with larger domain discrepancy (i.e. contribute more to the overall domain shift)
&lt;ul>
&lt;li>Assign local features with attention weights calculated using domain entropy&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;figure id="figure-the-domain-attention-mechanism-in-tasup3supn-thicker-arrows-correspond-to-larger-attention-weights">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/approach_TA3N_huff70fa82cfaf62901a8cb74823d96b36_122314_2000x2000_fit_lanczos_2.png" data-caption="The domain attention mechanism in TA3N (Thicker arrows correspond to larger attention weights).">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/approach_TA3N_huff70fa82cfaf62901a8cb74823d96b36_122314_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="100%" height="346">
&lt;/a>
&lt;figcaption>
The domain attention mechanism in TA&lt;sup>3&lt;/sup>N (Thicker arrows correspond to larger attention weights).
&lt;/figcaption>
&lt;/figure>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>We evaluate both small- and large-scale datasets, and achieve the state-of-the-art performance:&lt;/p>
&lt;ul>
&lt;li>Quantitative results:
&lt;figure id="figure-the-comparison-of-accuracy--with-other-approaches-on-_ucf-hmdbsubfullsub_-ucf----hmdb">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/result_TA3N_quantitative_hu86af7cd341c815527e27bee7cb48f53f_82705_2000x2000_fit_lanczos_2.png" data-caption="The comparison of accuracy (%) with other approaches on UCF-HMDBfull (UCF &amp;ndash;&amp;gt; HMDB).">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/result_TA3N_quantitative_hu86af7cd341c815527e27bee7cb48f53f_82705_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="50%" height="360">
&lt;/a>
&lt;figcaption>
The comparison of accuracy (%) with other approaches on &lt;em>UCF-HMDB&lt;sub>full&lt;/sub>&lt;/em> (UCF &amp;ndash;&amp;gt; HMDB).
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;li>Qualitative results:
&lt;figure id="figure-the-comparison-of-t-sne-visualization-between-image-based-da-and-our-approach-blue-source-orange-target">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/result_TA3N_qualitative_hu52c64adb84bea465dcafc27d3d518cd8_157293_2000x2000_fit_lanczos_2.png" data-caption="The comparison of t-SNE visualization between image-based DA and our approach (blue: source, orange: target).">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/result_TA3N_qualitative_hu52c64adb84bea465dcafc27d3d518cd8_157293_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="391">
&lt;/a>
&lt;figcaption>
The comparison of t-SNE visualization between image-based DA and our approach (blue: source, orange: target).
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;p>Please check our
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf" target="_blank" rel="noopener">paper&lt;/a> for more results.&lt;/p>
&lt;hr>
&lt;h2 id="videos">Videos&lt;/h2>
&lt;p>ICCV'19 Oral Presentation (please turn on closed captions):
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/j9cDuzmpYP8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>ICCV'19 Oral Presentation (officially recorded):
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8oUPyhwzIDo?start=6100&amp;amp;end=6395" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;h4 id="papers--code">Papers &amp;amp; Code&lt;/h4>
&lt;p>
&lt;a href="https://github.com/cmhungsteve/TA3N" target="_blank" rel="noopener">
&lt;figure id="figure-codehttpsgithubcomcmhungsteveta3n">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;figcaption>
&lt;a href="https://github.com/cmhungsteve/TA3N">&lt;strong>Code&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-iccv19httpsarxivorgpdf190712743pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="20%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf">ICCV'19&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://arxiv.org/pdf/1905.10861.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-cvprw19httpsarxivorgpdf190510861pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="20%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1905.10861.pdf">CVPRW'19&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;h4 id="presentations">Presentations&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://www.dropbox.com/s/7p1u8yro9hqseac/Oral_TA3N_ICCV_2019_mute.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-slides-iccv19">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/slides_ICCV2019_hubefd42348ee7371b35905da2500f1e83_82332_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="100%" height="720">
&lt;figcaption>
Slides (ICCV'19)
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.dropbox.com/s/nsb9bdfaz03mjv2/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-poster-iccv19">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/poster_ICCV2019_hub6005853782b9081f07ee0575514bfea_1443480_2000x2000_fit_lanczos_2.png" data-caption="Poster (ICCV&amp;#39;19)">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/poster_ICCV2019_hub6005853782b9081f07ee0575514bfea_1443480_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="70%" height="1776">
&lt;/a>
&lt;figcaption>
Poster (ICCV'19)
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;h4 id="other-links">Other Links&lt;/h4>
&lt;ul>
&lt;li>CVF Open Access
[
&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.html" target="_blank" rel="noopener">ICCV'19&lt;/a> ]&lt;/li>
&lt;li>IEEE Xplore
[
&lt;a href="https://ieeexplore.ieee.org/document/9008391" target="_blank" rel="noopener">ICCV'19&lt;/a> ]&lt;/li>
&lt;li>The workshop on Learning from Unlabeled Videos (LUV)
[
&lt;a href="https://sites.google.com/view/luv2019/program" target="_blank" rel="noopener">CVPRW'19&lt;/a> ]&lt;/li>
&lt;li>ML@GT [
&lt;a href="https://mlatgt.blog/2019/09/10/overcoming-large-scale-annotation-requirements-for-understanding-videos-in-the-wild" target="_blank" rel="noopener">Blog&lt;/a> ][
&lt;a href="https://mailchi.mp/4ed0cbf6a67d/iccv2019" target="_blank" rel="noopener">Article&lt;/a> ]&lt;/li>
&lt;/ul>
&lt;h4 id="datasets">Datasets&lt;/h4>
&lt;p>We propose two large-scale cross-domain action recognition datasets, &lt;em>UCF-HMDB&lt;sub>full&lt;/sub>&lt;/em> and &lt;em>Kinetics-Gameplay&lt;/em>.&lt;/p>
&lt;p>
&lt;figure id="figure-the-snapshots-in-the-_ucf-hmdbsubfullsub_-dataset">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/ucf_hmdb_hu5f00b314049d434c65c8e828bec18a41_384767_2000x2000_fit_lanczos_2.png" data-caption="The snapshots in the UCF-HMDBfull dataset.">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/ucf_hmdb_hu5f00b314049d434c65c8e828bec18a41_384767_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="589" height="50%">
&lt;/a>
&lt;figcaption>
The snapshots in the &lt;em>UCF-HMDB&lt;sub>full&lt;/sub>&lt;/em> dataset.
&lt;/figcaption>
&lt;/figure>
&lt;figure id="figure-the-snapshots-in-the-_kinetics-gameplay_-dataset">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/cdar/kinetics_gameplay_hu809b38b434046d8cf813c72d107bbaef_462778_2000x2000_fit_lanczos_2.png" data-caption="The snapshots in the Kinetics-Gameplay dataset.">
&lt;img data-src="https://minhungchen.netlify.app/project/cdar/kinetics_gameplay_hu809b38b434046d8cf813c72d107bbaef_462778_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="791" height="50%">
&lt;/a>
&lt;figcaption>
The snapshots in the &lt;em>Kinetics-Gameplay&lt;/em> dataset.
&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>To download the dataset, please visit our
&lt;a href="https://github.com/cmhungsteve/TA3N#dataset-preparation" target="_blank" rel="noopener">GitHub&lt;/a>. &lt;br>
Feel free to check our
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf" target="_blank" rel="noopener">paper&lt;/a> for more dataset details.&lt;/p>
&lt;hr>
&lt;h2 id="related-publications">Related Publications&lt;/h2>
&lt;p>If you find this project useful, please cite our papers:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen, and Jian Zheng, &amp;ldquo;Temporal Attentive Alignment for Large-Scale Video Domain Adaptation&amp;rdquo;,
&lt;a href="http://iccv2019.thecvf.com/" target="_blank" rel="noopener">&lt;em>IEEE International Conference on Computer Vision (ICCV)&lt;/em>, 2019&lt;/a> &lt;strong>[Oral (acceptance rate: 4.6%), travel grant awarded]&lt;/strong>.&lt;/li>
&lt;li>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, Zsolt Kira, and Ghassan AlRegib, &amp;ldquo;Temporal Attentive Alignment for Video Domain Adaptation&amp;rdquo;,
&lt;a href="https://sites.google.com/view/luv2019" target="_blank" rel="noopener">&lt;em>CVPR Workshop on Learning from Unlabeled Videos (LUV)&lt;/em>, 2019&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{chen2019temporal,
title={Temporal attentive alignment for large-scale video domain adaptation},
author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan and Yoo, Jaekwon and Chen, Ruxin and Zheng, Jian},
booktitle={IEEE International Conference on Computer Vision (ICCV)},
year={2019}
}
@article{chen2019taaan,
title={Temporal Attentive Alignment for Video Domain Adaptation},
author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan},
journal={CVPR Workshop on Learning from Unlabeled Videos},
year={2019},
url={https://arxiv.org/abs/1905.10861}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>Georgia Institute of Technology   &lt;sup>2&lt;/sup>Sony Interactive Entertainment LLC   &lt;sup>3&lt;/sup>Binghamton University &lt;br>&lt;/strong>
*work partially done as a SIE intern&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">
&lt;figure id="figure-zsolt-kirasup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_zk.jpg" alt="" width="100%" >
&lt;figcaption>
Zsolt Kira&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregibsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/jaekwon-yoo-8685862b/" target="_blank" rel="noopener">
&lt;figure id="figure-jaekwon-yoosup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Jaekwon Yoo&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/ruxin-chen-991477119/" target="_blank" rel="noopener">
&lt;figure id="figure-ruxin-chensup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Ruxin Chen&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?hl=en&amp;amp;user=5YR6dTEAAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate" target="_blank" rel="noopener">
&lt;figure id="figure-jian-zhengsup3sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_jz.jpg" alt="" width="100%" >
&lt;figcaption>
Jian Zheng&lt;sup>3&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Traffic Sign Detection under Challenging Conditions</title><link>https://minhungchen.netlify.app/project/curetsd/</link><pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/project/curetsd/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Existing traffic sign datasets are limited in terms of type and severity of challenging conditions. Metadata corresponding to these conditions are unavailable and it is not possible to investigate the effect of a single factor because of simultaneous changes in numerous conditions. Therefore, we introduce the &lt;strong>CURE-TSD&lt;/strong> dataset, including various challenging conditions with both real and synthetic data.&lt;/p>
&lt;!-- ---
## Challenges
(coming soon)
---
## Our Approaches
(coming soon)
-->
&lt;hr>
&lt;h2 id="dataset-overview">Dataset Overview&lt;/h2>
&lt;ul>
&lt;li>Video number: 5733 videos&lt;/li>
&lt;li>Video length: 300 frames/video (~1.7M total frames in the dataset)&lt;/li>
&lt;li>Challenge types: 12&lt;/li>
&lt;li>Challenge levels: 5
&lt;figure id="figure-challenging-conditions-horizontal-challenge-levels-vertical-challenge-types">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/curetsd/curetsd_challenges_hu05eb8e363895033479e945a3fafedd1b_1062714_2000x2000_fit_lanczos_2.png" data-caption="Challenging conditions (horizontal: challenge levels. vertical: challenge types).">
&lt;img data-src="https://minhungchen.netlify.app/project/curetsd/curetsd_challenges_hu05eb8e363895033479e945a3fafedd1b_1062714_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="1000">
&lt;/a>
&lt;figcaption>
Challenging conditions (horizontal: challenge levels. vertical: challenge types).
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;li>Traffic sign types: 14
&lt;figure id="figure-traffic-signs-row-1-real-signs-row-2-synthetic-signs">
&lt;a data-fancybox="" href="https://minhungchen.netlify.app/project/curetsd/sign_types_huff92360b86bedc6e357bd78014ffbff6_209212_2000x2000_fit_lanczos_2.png" data-caption="Traffic signs (Row 1: real signs. Row 2: synthetic signs).">
&lt;img data-src="https://minhungchen.netlify.app/project/curetsd/sign_types_huff92360b86bedc6e357bd78014ffbff6_209212_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="154">
&lt;/a>
&lt;figcaption>
Traffic signs (Row 1: real signs. Row 2: synthetic signs).
&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;/ul>
&lt;h3 id="demo-videos">Demo Videos&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Real data:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8V1LcpDlmjA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Synthetic data:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/bKnlJ_EWS8Q" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Please check our
&lt;a href="https://arxiv.org/pdf/1902.06857.pdf" target="_blank" rel="noopener">paper&lt;/a> for more results.&lt;/p>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;h4 id="papers--github">Papers &amp;amp; GitHub&lt;/h4>
&lt;p>
&lt;a href="https://github.com/olivesgatech/CURE-TSD" target="_blank" rel="noopener">
&lt;figure id="figure-githubhttpsgithubcomolivesgatechcure-tsd">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;figcaption>
&lt;a href="https://github.com/olivesgatech/CURE-TSD">&lt;strong>GitHub&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://arxiv.org/pdf/1908.11262.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-tits19httpsarxivorgpdf190811262pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="20%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1908.11262.pdf">TITS'19&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://arxiv.org/pdf/1902.06857.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-arxiv19httpsarxivorgpdf190206857pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="20%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1902.06857.pdf">arXiv'19&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;h4 id="download">Download&lt;/h4>
&lt;p>To download the dataset, please visit our
&lt;a href="https://github.com/olivesgatech/CURE-TSD" target="_blank" rel="noopener">GitHub&lt;/a> or
&lt;a href="https://ieee-dataport.org/open-access/cure-tsd-challenging-unreal-and-real-environment-traffic-sign-detection" target="_blank" rel="noopener">IEEE DataPort&lt;/a>.&lt;/p>
&lt;h4 id="other-links">Other Links&lt;/h4>
&lt;ul>
&lt;li>IEEE Xplore
[
&lt;a href="https://ieeexplore.ieee.org/document/8793235" target="_blank" rel="noopener">TITS'19&lt;/a> ]&lt;/li>
&lt;li>IEEE DataPort
[
&lt;a href="https://ieee-dataport.org/open-access/cure-tsd-challenging-unreal-and-real-environment-traffic-sign-detection" target="_blank" rel="noopener">CURE-TSD&lt;/a> ]&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="related-publications">Related Publications&lt;/h2>
&lt;p>If you find this project useful, please cite our papers (*equal contribution):&lt;/p>
&lt;ul>
&lt;li>Dogancan Temel,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, and Ghassan AlRegib, &amp;ldquo;Traffic Sign Detection under Challenging Conditions: A Deeper Look Into Performance Variations and Spectral Characteristics&amp;rdquo;,
&lt;a href="https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=6979" target="_blank" rel="noopener">&lt;em>IEEE Transactions on Intelligent Transportation Systems (TITS)&lt;/em>, 2019&lt;/a>.&lt;/li>
&lt;li>Dogancan Temel, Tariq Alshawi*,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen*&lt;/strong>&lt;/a>, and Ghassan AlRegib, &amp;ldquo;Challenging Environments for Traffic Sign Detection: Reliability Assessment under Inclement Conditions&amp;rdquo;, &lt;em>arXiv:1902.06857&lt;/em>, 2019.&lt;/li>
&lt;/ul>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@article{temel2019traffic,
title={Traffic sign detection under challenging conditions: A deeper look into performance variations and spectral characteristics},
author={Temel, Dogancan and Chen, Min-Hung and AlRegib, Ghassan},
journal={IEEE Transactions on Intelligent Transportation Systems (TITS)},
year={2019},
publisher={IEEE}
}
@article{temel2019challenging,
title={Challenging environments for traffic sign detection: Reliability assessment under inclement conditions},
author={Temel, Dogancan and Alshawi, Tariq and Chen, Min-Hung and AlRegib, Ghassan},
journal={arXiv preprint arXiv:1902.06857},
year={2019},
url={https://arxiv.org/abs/1902.06857}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>Georgia Institute of Technology &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="http://cantemel.com/" target="_blank" rel="noopener">
&lt;figure id="figure-dogancan-temel">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ct.jpg" alt="" width="100%" >
&lt;figcaption>
Dogancan Temel
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/tariq-alshawi/" target="_blank" rel="noopener">
&lt;figure id="figure-tariq-alshawi">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ta.jpg" alt="" width="100%" >
&lt;figcaption>
Tariq Alshawi
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregib">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation</title><link>https://minhungchen.netlify.app/publication/sstda/</link><pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/sstda/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="videos">Videos&lt;/h2>
&lt;p>Overview introduction with less technical details:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/HxPYhOZco-4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>1-min talk video:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/sKCXZksFOWA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;h2 id="hahahugoshortcode-s4-hbhb">5-min talk video:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/16OTdVeKahs" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/h2>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;p>
&lt;a href="https://github.com/cmhungsteve/SSTDA" target="_blank" rel="noopener">
&lt;figure id="figure-codehttpsgithubcomcmhungstevesstda">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;figcaption>
&lt;a href="https://github.com/cmhungsteve/SSTDA">&lt;strong>Code&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://arxiv.org/pdf/2003.02824.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-paperhttpsarxivorgpdf200302824pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/2003.02824.pdf">Paper&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.dropbox.com/s/xj37x5xpmqg70ln/Spotlight_SSTDA_CVPR_2020_mute.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-slides-1-min-talkhttpswwwdropboxcomsxj37x5xpmqg70lnspotlight_sstda_cvpr_2020_mutepdfdl0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/xj37x5xpmqg70ln/Spotlight_SSTDA_CVPR_2020_mute.pdf?dl=0">Slides (1-min talk)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.dropbox.com/s/o2d192zb3his06r/Oral_SSTDA_CVPR_2020_mute.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-slides-5-min-talkhttpswwwdropboxcomso2d192zb3his06roral_sstda_cvpr_2020_mutepdfdl0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/o2d192zb3his06r/Oral_SSTDA_CVPR_2020_mute.pdf?dl=0">Slides (5-min talk)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.dropbox.com/s/fm6cmwgzrjslvbd/CVPR2020_Steve_SSTDA_poster_v1.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-posterhttpswwwdropboxcomsfm6cmwgzrjslvbdcvpr2020_steve_sstda_poster_v1pdfdl0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/fm6cmwgzrjslvbd/CVPR2020_Steve_SSTDA_poster_v1.pdf?dl=0">Poster&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Action_Segmentation_With_Joint_Self-Supervised_Temporal_Domain_Adaptation_CVPR_2020_paper.html" target="_blank" rel="noopener">CVF Open Access&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://ieeexplore.ieee.org/document/9157452" target="_blank" rel="noopener">IEEE Xplore&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://mlgt-at-cvpr-2020.mailchimpsites.com/" target="_blank" rel="noopener">GT@CVPR'20&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://dblp.org/pers/l/Li:Baopu.html" target="_blank" rel="noopener">Baopu Li&lt;/a>,
&lt;a href="https://scholar.google.com/citations?user=WiAQC68AAAAJ" target="_blank" rel="noopener">Yingze Bao&lt;/a>,
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>, and
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">Zsolt Kira&lt;/a>, &amp;ldquo;Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation&amp;rdquo;,
&lt;a href="http://cvpr2020.thecvf.com/" target="_blank" rel="noopener">&lt;em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em>, 2020&lt;/a>.&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{chen2020action,
title={Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation},
author={Chen, Min-Hung and Li, Baopu and Bao, Yingze and AlRegib, Ghassan and Kira, Zsolt},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2020}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>Georgia Institute of Technology   &lt;sup>2&lt;/sup>Baidu USA &lt;br>&lt;/strong>
*work done during an internship at Baidu USA&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://dblp.org/pers/l/Li:Baopu.html" target="_blank" rel="noopener">
&lt;figure id="figure-baopu-lisup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_bl.jpg" alt="" width="100%" >
&lt;figcaption>
Baopu Li&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=WiAQC68AAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-yingze-baosup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_yb.jpg" alt="" width="100%" >
&lt;figcaption>
Yingze Bao&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregibsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">
&lt;figure id="figure-zsolt-kirasup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_zk.jpg" alt="" width="100%" >
&lt;figcaption>
Zsolt Kira&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Interpretable Self-Attention Temporal Reasoning for Driving Behavior Understanding</title><link>https://minhungchen.netlify.app/publication/trb/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/trb/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="resources">Resources&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-paper-arxivhttpsarxivorgftparxivpapers1911191102172pdf">
&lt;a href="https://arxiv.org/ftp/arxiv/papers/1911/1911.02172.pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="25%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://arxiv.org/ftp/arxiv/papers/1911/1911.02172.pdf">Paper (arXiv)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-slideshttpssigportorgsitesdefaultfilesdocsinterpretable20self-attention20temporal20reasoning20for20driving20behavior20understandingpdf">
&lt;a href="https://sigport.org/sites/default/files/docs/INTERPRETABLE%20SELF-ATTENTION%20TEMPORAL%20REASONING%20FOR%20DRIVING%20BEHAVIOR%20UNDERSTANDING.pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="25%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://sigport.org/sites/default/files/docs/INTERPRETABLE%20SELF-ATTENTION%20TEMPORAL%20REASONING%20FOR%20DRIVING%20BEHAVIOR%20UNDERSTANDING.pdf">Slides&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://ieeexplore.ieee.org/abstract/document/9053783" target="_blank" rel="noopener">IEEE Xplore&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="https://www.linkedin.com/in/ejliu617/" target="_blank" rel="noopener">Yi-Chieh Liu*&lt;/a>,
&lt;a href="https://www.linkedin.com/in/yhsieh37/" target="_blank" rel="noopener">Yung-An Hsieh*&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://www.linkedin.com/in/huckyang/" target="_blank" rel="noopener">Chao-Han (Huck) Yang&lt;/a>,
&lt;a href="https://scholar.google.se/citations?hl=en&amp;amp;user=_DUppAgAAAAJ" target="_blank" rel="noopener">Jesper Tegner&lt;/a>, and
&lt;a href="https://scholar.google.com/citations?user=S41zwP4AAAAJ" target="_blank" rel="noopener">Yi-Chang (James) Tsai&lt;/a>, &amp;ldquo;Interpretable Self-Attention Temporal Reasoning for Driving Behavior Understanding&amp;rdquo;,
&lt;a href="https://2020.ieeeicassp.org/" target="_blank" rel="noopener">&lt;em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)&lt;/em>, 2020&lt;/a>. (*equal contribution)&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{liu2020interpretable,
title={Interpretable Self-Attention Temporal Reasoning for Driving Behavior Understanding},
author={Liu, Yi-Chieh and Hsieh, Yung-An and Chen, Min-Hung and Yang, C-H Huck and Tegner, Jesper and Tsai, Y-C James},
booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
year={2020}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>Georgia Institute of Technology   &lt;sup>2&lt;/sup>KAUST &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://www.linkedin.com/in/ejliu617/" target="_blank" rel="noopener">
&lt;figure id="figure-yi-chieh-liusup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ycl.jpg" alt="" width="100%" >
&lt;figcaption>
Yi-Chieh Liu&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/yhsieh37/" target="_blank" rel="noopener">
&lt;figure id="figure-yung-an-hsiehsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_yah.jpg" alt="" width="100%" >
&lt;figcaption>
Yung-An Hsieh&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/huckyang/" target="_blank" rel="noopener">
&lt;figure id="figure-huck-yangsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_chy.jpg" alt="" width="100%" >
&lt;figcaption>
Huck Yang&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.se/citations?hl=en&amp;amp;user=_DUppAgAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-jesper-tegnersup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_jt.jpg" alt="" width="100%" >
&lt;figcaption>
Jesper Tegner&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=S41zwP4AAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-james-tsaisup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_yct.jpg" alt="" width="100%" >
&lt;figcaption>
James Tsai&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Action Segmentation with Mixed Temporal Domain Adaptation</title><link>https://minhungchen.netlify.app/publication/mtda/</link><pubDate>Mon, 02 Mar 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/mtda/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="videos">Videos&lt;/h2>
&lt;h2 id="hahahugoshortcode-s2-hbhb">
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/RkdhHDvj4UA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/h2>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-paperhttpopenaccessthecvfcomcontent_wacv_2020paperschen_action_segmentation_with_mixed_temporal_domain_adaptation_wacv_2020_paperpdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;figcaption>
&lt;a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.pdf">Paper&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.dropbox.com/s/24yi5ygs68mqbhc/Oral_MTDA_WACV_2020_mute.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-slideshttpswwwdropboxcoms24yi5ygs68mqbhcoral_mtda_wacv_2020_mutepdfdl0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/24yi5ygs68mqbhc/Oral_MTDA_WACV_2020_mute.pdf?dl=0">Slides&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.dropbox.com/s/74or0hkkodhiqzc/WACV2020_Steve_MTDA_poster_v1.pdf?dl=0" target="_blank" rel="noopener">
&lt;figure id="figure-posterhttpswwwdropboxcoms74or0hkkodhiqzcwacv2020_steve_mtda_poster_v1pdfdl0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/74or0hkkodhiqzc/WACV2020_Steve_MTDA_poster_v1.pdf?dl=0">Poster&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="http://openaccess.thecvf.com/content_WACV_2020/html/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.html" target="_blank" rel="noopener">CVF Open Access&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://ieeexplore.ieee.org/document/9093535" target="_blank" rel="noopener">IEEE Xplore&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://dblp.org/pers/l/Li:Baopu.html" target="_blank" rel="noopener">Baopu Li&lt;/a>,
&lt;a href="https://scholar.google.com/citations?user=WiAQC68AAAAJ" target="_blank" rel="noopener">Yingze Bao&lt;/a>, and
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>, &amp;ldquo;Action Segmentation with Mixed Temporal Domain Adaptation&amp;rdquo;,
&lt;a href="http://wacv20.wacv.net/" target="_blank" rel="noopener">&lt;em>IEEE Winter Conference on Applications of Computer Vision (WACV)&lt;/em>, 2020&lt;/a>.&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{chen2020mixed,
title={Action Segmentation with Mixed Temporal Domain Adaptation},
author={Chen, Min-Hung and Li, Baopu and Bao, Yingze and AlRegib, Ghassan},
booktitle={IEEE Winter Conference on Applications of Computer Vision (WACV)},
year={2020}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>Georgia Institute of Technology   &lt;sup>2&lt;/sup>Baidu USA &lt;br>&lt;/strong>
*work done during an internship at Baidu USA&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://dblp.org/pers/l/Li:Baopu.html" target="_blank" rel="noopener">
&lt;figure id="figure-baopu-lisup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_bl.jpg" alt="" width="100%" >
&lt;figcaption>
Baopu Li&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=WiAQC68AAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-yingze-baosup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_yb.jpg" alt="" width="100%" >
&lt;figcaption>
Yingze Bao&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregibsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Color learning</title><link>https://minhungchen.netlify.app/publication/colortsd/</link><pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/colortsd/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://www.linkedin.com/in/david-mccreadie-4b603643/" target="_blank" rel="noopener">David McCreadie&lt;/a>, and
&lt;a href="https://www.linkedin.com/in/bostondaniel/" target="_blank" rel="noopener">Daniel Lewis Boston&lt;/a>, &amp;ldquo;Color learning&amp;rdquo;,
&lt;a href="https://patents.google.com/patent/US10552692B2" target="_blank" rel="noopener">&lt;em>US Patent 10552692&lt;/em>, 2020&lt;/a>.&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@misc{alregib2020color,
title={Color learning},
author={AlRegib, Ghassan and Chen, Min-Hung and McCreadie, David and },
year={2020},
month=February,
publisher={Google Patents},
note={US Patent 10552692}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>Georgia Institute of Technology   &lt;sup>2&lt;/sup>Ford Motor Company &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregibsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/david-mccreadie-4b603643/" target="_blank" rel="noopener">
&lt;figure id="figure-david-mccreadiesup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
David McCreadie&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/bostondaniel/" target="_blank" rel="noopener">
&lt;figure id="figure-daniel-bostonsup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_db.jpg" alt="" width="100%" >
&lt;figcaption>
Daniel Boston&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</title><link>https://minhungchen.netlify.app/publication/ta3n/</link><pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/ta3n/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="videos">Videos&lt;/h2>
&lt;p>Oral Video (please turn on closed captions):
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/j9cDuzmpYP8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;figure id="figure-codehttpsgithubcomcmhungsteveta3n">
&lt;a href="https://github.com/cmhungsteve/TA3N">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/cmhungsteve/TA3N">&lt;strong>Code&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-paperhttpsarxivorgpdf190712743pdf">
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1907.12743.pdf">Paper&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-slideshttpswwwdropboxcomss9ud77a1zt0vqbnoral_ta3n_iccv_2019_mutepdfdl0">
&lt;a href="https://www.dropbox.com/s/s9ud77a1zt0vqbn/Oral_TA3N_ICCV_2019_mute.pdf?dl=0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/s9ud77a1zt0vqbn/Oral_TA3N_ICCV_2019_mute.pdf?dl=0">Slides&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-posterhttpswwwdropboxcomsg16v02k9sgn1xibiccv2019_steve_ta3n_poster_v1_2pdfdl0">
&lt;a href="https://www.dropbox.com/s/g16v02k9sgn1xib/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/g16v02k9sgn1xib/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0">Poster&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.html" target="_blank" rel="noopener">CVF Open Access&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://ieeexplore.ieee.org/document/9008391" target="_blank" rel="noopener">IEEE Xplore&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://mlatgt.blog/2019/09/10/overcoming-large-scale-annotation-requirements-for-understanding-videos-in-the-wild" target="_blank" rel="noopener">ML@GT Blog&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://mailchi.mp/4ed0cbf6a67d/iccv2019" target="_blank" rel="noopener">GT@ICCV'19&lt;/a>&lt;/li>
&lt;li>CVPR workshop on Learning from Unlabeled Videos (LUV) [
&lt;a href="https://sites.google.com/view/luv2019/program" target="_blank" rel="noopener">CVPRW'19&lt;/a> ]&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;ul>
&lt;li>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">Zsolt Kira&lt;/a>,
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>,
&lt;a href="https://www.linkedin.com/in/jaekwon-yoo-8685862b/" target="_blank" rel="noopener">Jaekwon Yoo&lt;/a>,
&lt;a href="%28https://www.linkedin.com/in/ruxin-chen-991477119/%29">Ruxin Chen&lt;/a>, and
&lt;a href="https://scholar.google.com/citations?user=5YR6dTEAAAAJ" target="_blank" rel="noopener">Jian Zheng&lt;/a>, &amp;ldquo;Temporal Attentive Alignment for Large-Scale Video Domain Adaptation&amp;rdquo;,
&lt;a href="http://iccv2019.thecvf.com/" target="_blank" rel="noopener">&lt;em>IEEE International Conference on Computer Vision (ICCV)&lt;/em>, 2019&lt;/a> &lt;strong>[Oral (acceptance rate: 4.6%), travel grant awarded]&lt;/strong>.&lt;/li>
&lt;li>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>,
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">Zsolt Kira&lt;/a>, and
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>, &amp;ldquo;Temporal Attentive Alignment for Video Domain Adaptation&amp;rdquo;,
&lt;a href="https://sites.google.com/view/luv2019" target="_blank" rel="noopener">&lt;em>CVPR Workshop on Learning from Unlabeled Videos (LUV)&lt;/em>, 2019&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{chen2019temporal,
title={Temporal attentive alignment for large-scale video domain adaptation},
author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan and Yoo, Jaekwon and Chen, Ruxin and Zheng, Jian},
booktitle={IEEE International Conference on Computer Vision (ICCV)},
year={2019}
}
@article{chen2019taaan,
title={Temporal Attentive Alignment for Video Domain Adaptation},
author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan},
journal={CVPR Workshop on Learning from Unlabeled Videos},
year={2019},
url={https://arxiv.org/abs/1905.10861}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>&lt;sup>1&lt;/sup>Georgia Institute of Technology   &lt;sup>2&lt;/sup>Sony Interactive Entertainment LLC   &lt;sup>3&lt;/sup>Binghamton University &lt;br>&lt;/strong>
*work partially done as a SIE intern&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chensup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen&lt;sup>1&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">
&lt;figure id="figure-zsolt-kirasup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_zk.jpg" alt="" width="100%" >
&lt;figcaption>
Zsolt Kira&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregibsup1sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib&lt;sup>1&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/jaekwon-yoo-8685862b/" target="_blank" rel="noopener">
&lt;figure id="figure-jaekwon-yoosup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Jaekwon Yoo&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/ruxin-chen-991477119/" target="_blank" rel="noopener">
&lt;figure id="figure-ruxin-chensup2sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_unknown.jpg" alt="" width="100%" >
&lt;figcaption>
Ruxin Chen&lt;sup>2&lt;/sup>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=5YR6dTEAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-jian-zhengsup3sup">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_jz.jpg" alt="" width="100%" >
&lt;figcaption>
Jian Zheng&lt;sup>3&lt;/sup>*
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Traffic Sign Detection Under Challenging Conditions: A Deeper Look into Performance Variations and Spectral Characteristics</title><link>https://minhungchen.netlify.app/publication/curetsd/</link><pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/curetsd/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="demo-videos">Demo Videos&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Real data:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8V1LcpDlmjA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Synthetic data:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/bKnlJ_EWS8Q" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-githubhttpsgithubcomolivesgatechcure-tsd">
&lt;a href="https://github.com/olivesgatech/CURE-TSD">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="65%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/olivesgatech/CURE-TSD">&lt;strong>GitHub&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;a href="https://arxiv.org/pdf/1908.11262.pdf" target="_blank" rel="noopener">
&lt;figure id="figure-paper-arxivhttpsarxivorgpdf190811262pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="15%" >
&lt;figcaption>
&lt;a href="https://arxiv.org/pdf/1908.11262.pdf">Paper (arXiv)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://ieeexplore.ieee.org/document/8793235" target="_blank" rel="noopener">IEEE Xplore&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="http://cantemel.com/" target="_blank" rel="noopener">Dogancan Temel&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, and
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>, &amp;ldquo;Traffic Sign Detection under Challenging Conditions: A Deeper Look Into Performance Variations and Spectral Characteristics&amp;rdquo;,
&lt;a href="https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=6979" target="_blank" rel="noopener">&lt;em>IEEE Transactions on Intelligent Transportation Systems (TITS)&lt;/em>, 2019&lt;/a>.&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@article{temel2019traffic,
title={Traffic sign detection under challenging conditions: A deeper look into performance variations and spectral characteristics},
author={Temel, Dogancan and Chen, Min-Hung and AlRegib, Ghassan},
journal={IEEE Transactions on Intelligent Transportation Systems (TITS)},
year={2019},
publisher={IEEE}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>Georgia Institute of Technology &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="http://cantemel.com/" target="_blank" rel="noopener">
&lt;figure id="figure-dogancan-temel">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ct.jpg" alt="" width="100%" >
&lt;figcaption>
Dogancan Temel
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregib">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Display Jupyter Notebooks with Academic</title><link>https://minhungchen.netlify.app/post/jupyter/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/post/jupyter/</guid><description>&lt;pre>&lt;code class="language-python">from IPython.core.display import Image
Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="./index_1_0.png" alt="png">&lt;/p>
&lt;pre>&lt;code class="language-python">print(&amp;quot;Welcome to Academic!&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>Welcome to Academic!
&lt;/code>&lt;/pre>
&lt;h2 id="install-python-and-jupyterlab">Install Python and JupyterLab&lt;/h2>
&lt;p>
&lt;a href="https://www.anaconda.com/distribution/#download-section" target="_blank" rel="noopener">Install Anaconda&lt;/a> which includes Python 3 and JupyterLab.&lt;/p>
&lt;p>Alternatively, install JupyterLab with &lt;code>pip3 install jupyterlab&lt;/code>.&lt;/p>
&lt;h2 id="create-or-upload-a-jupyter-notebook">Create or upload a Jupyter notebook&lt;/h2>
&lt;p>Run the following commands in your Terminal, substituting &lt;code>&amp;lt;MY-WEBSITE-FOLDER&amp;gt;&lt;/code> and &lt;code>&amp;lt;SHORT-POST-TITLE&amp;gt;&lt;/code> with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:&lt;/p>
&lt;pre>&lt;code class="language-bash">mkdir -p &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
cd &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
jupyter lab index.ipynb
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>jupyter&lt;/code> command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.&lt;/p>
&lt;h2 id="edit-your-post-metadata">Edit your post metadata&lt;/h2>
&lt;p>The first cell of your Jupter notebook will contain your post metadata (
&lt;a href="https://sourcethemes.com/academic/docs/front-matter/" target="_blank" rel="noopener">front matter&lt;/a>).&lt;/p>
&lt;p>In Jupter, choose &lt;em>Markdown&lt;/em> as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:&lt;/p>
&lt;pre>&lt;code>---
title: My post's title
date: 2019-09-01
# Put any other Academic metadata here...
---
&lt;/code>&lt;/pre>
&lt;p>Edit the metadata of your post, using the
&lt;a href="https://sourcethemes.com/academic/docs/managing-content" target="_blank" rel="noopener">documentation&lt;/a> as a guide to the available options.&lt;/p>
&lt;p>To set a
&lt;a href="https://sourcethemes.com/academic/docs/managing-content/#featured-image" target="_blank" rel="noopener">featured image&lt;/a>, place an image named &lt;code>featured&lt;/code> into your post&amp;rsquo;s folder.&lt;/p>
&lt;p>For other tips, such as using math, see the guide on
&lt;a href="https://sourcethemes.com/academic/docs/writing-markdown-latex/" target="_blank" rel="noopener">writing content with Academic&lt;/a>.&lt;/p>
&lt;h2 id="convert-notebook-to-markdown">Convert notebook to Markdown&lt;/h2>
&lt;pre>&lt;code class="language-bash">jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.
&lt;/code>&lt;/pre>
&lt;h2 id="example">Example&lt;/h2>
&lt;p>This post was created with Jupyter. The orginal files can be found at &lt;a href="https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter">https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&lt;/a>&lt;/p></description></item><item><title>TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition</title><link>https://minhungchen.netlify.app/publication/tslstm/</link><pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/tslstm/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="demo-videos">Demo Videos&lt;/h2>
&lt;p>Please check our
&lt;a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN" target="_blank" rel="noopener">GitHub&lt;/a>.&lt;/p>
&lt;hr>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;figure id="figure-codehttpsgithubcomchihyaomaactivity-recognition-with-cnn-and-rnn">
&lt;a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN">
&lt;img src="https://minhungchen.netlify.app/img/github_icon.png" alt="" width="15%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN">&lt;strong>Code&lt;/strong>&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-paper-arxivhttpsarxivorgabs170310667">
&lt;a href="https://arxiv.org/abs/1703.10667">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="25%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://arxiv.org/abs/1703.10667">Paper (arXiv)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-poster-mlgthttpswwwdropboxcoms1c7fsgr3ef1x9v6poster_tslstm-teminception_20170417pdfdl0">
&lt;a href="https://www.dropbox.com/s/1c7fsgr3ef1x9v6/poster_TSLSTM-TemInception_20170417.pdf?dl=0">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="25%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://www.dropbox.com/s/1c7fsgr3ef1x9v6/poster_TSLSTM-TemInception_20170417.pdf?dl=0">Poster (ML@GT)&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0923596518304922" target="_blank" rel="noopener">SPIC Journal&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="https://chihyaoma.github.io/" target="_blank" rel="noopener">Chih-Yao Ma*&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen*&lt;/strong>&lt;/a>,
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">Zsolt Kira&lt;/a>, and
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">Ghassan AlRegib&lt;/a>, &amp;ldquo;TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition&amp;rdquo;,
&lt;a href="https://www.sciencedirect.com/journal/signal-processing-image-communication/vol/71/" target="_blank" rel="noopener">&lt;em>Signal Processing: Image Communication (SPIC)&lt;/em>, 2019&lt;/a>. (*equal contribution)&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@article{ma2019ts,
title={TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition},
author={Ma, Chih-Yao and Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan},
journal={Signal Processing: Image Communication},
volume={71},
pages={76--87},
year={2019},
publisher={Elsevier}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>Georgia Institute of Technology &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://chihyaoma.github.io/" target="_blank" rel="noopener">
&lt;figure id="figure-chih-yao-ma">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_cym.jpg" alt="" width="100%" >
&lt;figcaption>
Chih-Yao Ma
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.cc.gatech.edu/~zk15/" target="_blank" rel="noopener">
&lt;figure id="figure-zsolt-kira">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_zk.jpg" alt="" width="100%" >
&lt;figcaption>
Zsolt Kira
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://ghassanalregib.info/" target="_blank" rel="noopener">
&lt;figure id="figure-ghassan-alregib">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_ga.jpg" alt="" width="100%" >
&lt;figcaption>
Ghassan AlRegib
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item><item><title>Academic: the website builder for Hugo</title><link>https://minhungchen.netlify.app/post/getting-started/</link><pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/post/getting-started/</guid><description>&lt;p>&lt;strong>Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 &lt;em>widgets&lt;/em>, &lt;em>themes&lt;/em>, and &lt;em>language packs&lt;/em> included!&lt;/strong>&lt;/p>
&lt;p>
&lt;a href="https://academic-demo.netlify.com/" target="_blank" rel="noopener">Check out the latest &lt;strong>demo&lt;/strong>&lt;/a> of what you&amp;rsquo;ll get in less than 10 minutes, or
&lt;a href="https://sourcethemes.com/academic/#expo" target="_blank" rel="noopener">view the &lt;strong>showcase&lt;/strong>&lt;/a> of personal, project, and business sites.&lt;/p>
&lt;ul>
&lt;li>👉
&lt;a href="#install">&lt;strong>Get Started&lt;/strong>&lt;/a>&lt;/li>
&lt;li>📚
&lt;a href="https://sourcethemes.com/academic/docs/" target="_blank" rel="noopener">View the &lt;strong>documentation&lt;/strong>&lt;/a>&lt;/li>
&lt;li>💬
&lt;a href="https://discourse.gohugo.io" target="_blank" rel="noopener">&lt;strong>Ask a question&lt;/strong> on the forum&lt;/a>&lt;/li>
&lt;li>👥
&lt;a href="https://spectrum.chat/academic" target="_blank" rel="noopener">Chat with the &lt;strong>community&lt;/strong>&lt;/a>&lt;/li>
&lt;li>🐦 Twitter:
&lt;a href="https://twitter.com/source_themes" target="_blank" rel="noopener">@source_themes&lt;/a>
&lt;a href="https://twitter.com/GeorgeCushen" target="_blank" rel="noopener">@GeorgeCushen&lt;/a>
&lt;a href="https://twitter.com/search?q=%23MadeWithAcademic&amp;amp;src=typd" target="_blank" rel="noopener">#MadeWithAcademic&lt;/a>&lt;/li>
&lt;li>💡
&lt;a href="https://github.com/gcushen/hugo-academic/issues" target="_blank" rel="noopener">Request a &lt;strong>feature&lt;/strong> or report a &lt;strong>bug&lt;/strong>&lt;/a>&lt;/li>
&lt;li>⬆️ &lt;strong>Updating?&lt;/strong> View the
&lt;a href="https://sourcethemes.com/academic/docs/update/" target="_blank" rel="noopener">Update Guide&lt;/a> and
&lt;a href="https://sourcethemes.com/academic/updates/" target="_blank" rel="noopener">Release Notes&lt;/a>&lt;/li>
&lt;li>❤️ &lt;strong>Support development&lt;/strong> of Academic:
&lt;ul>
&lt;li>☕️
&lt;a href="https://paypal.me/cushen" target="_blank" rel="noopener">&lt;strong>Donate a coffee&lt;/strong>&lt;/a>&lt;/li>
&lt;li>💵
&lt;a href="https://www.patreon.com/cushen" target="_blank" rel="noopener">Become a backer on &lt;strong>Patreon&lt;/strong>&lt;/a>&lt;/li>
&lt;li>🖼️
&lt;a href="https://www.redbubble.com/people/neutreno/works/34387919-academic" target="_blank" rel="noopener">Decorate your laptop or journal with an Academic &lt;strong>sticker&lt;/strong>&lt;/a>&lt;/li>
&lt;li>👕
&lt;a href="https://academic.threadless.com/" target="_blank" rel="noopener">Wear the &lt;strong>T-shirt&lt;/strong>&lt;/a>&lt;/li>
&lt;li>👩‍💻
&lt;a href="https://sourcethemes.com/academic/docs/contribute/" target="_blank" rel="noopener">&lt;strong>Contribute&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;figure id="figure-academic-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device">
&lt;a data-fancybox="" href="https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png" data-caption="Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.">
&lt;img src="https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png" alt="" >
&lt;/a>
&lt;figcaption>
Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.
&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;strong>Key features:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Page builder&lt;/strong> - Create &lt;em>anything&lt;/em> with
&lt;a href="https://sourcethemes.com/academic/docs/page-builder/" target="_blank" rel="noopener">&lt;strong>widgets&lt;/strong>&lt;/a> and
&lt;a href="https://sourcethemes.com/academic/docs/writing-markdown-latex/" target="_blank" rel="noopener">&lt;strong>elements&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;strong>Edit any type of content&lt;/strong> - Blog posts, publications, talks, slides, projects, and more!&lt;/li>
&lt;li>&lt;strong>Create content&lt;/strong> in
&lt;a href="https://sourcethemes.com/academic/docs/writing-markdown-latex/" target="_blank" rel="noopener">&lt;strong>Markdown&lt;/strong>&lt;/a>,
&lt;a href="https://sourcethemes.com/academic/docs/jupyter/" target="_blank" rel="noopener">&lt;strong>Jupyter&lt;/strong>&lt;/a>, or
&lt;a href="https://sourcethemes.com/academic/docs/install/#install-with-rstudio" target="_blank" rel="noopener">&lt;strong>RStudio&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;strong>Plugin System&lt;/strong> - Fully customizable
&lt;a href="https://sourcethemes.com/academic/themes/" target="_blank" rel="noopener">&lt;strong>color&lt;/strong> and &lt;strong>font themes&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;strong>Display Code and Math&lt;/strong> - Code highlighting and
&lt;a href="https://en.wikibooks.org/wiki/LaTeX/Mathematics" target="_blank" rel="noopener">LaTeX math&lt;/a> supported&lt;/li>
&lt;li>&lt;strong>Integrations&lt;/strong> -
&lt;a href="https://analytics.google.com" target="_blank" rel="noopener">Google Analytics&lt;/a>,
&lt;a href="https://disqus.com" target="_blank" rel="noopener">Disqus commenting&lt;/a>, Maps, Contact Forms, and more!&lt;/li>
&lt;li>&lt;strong>Beautiful Site&lt;/strong> - Simple and refreshing one page design&lt;/li>
&lt;li>&lt;strong>Industry-Leading SEO&lt;/strong> - Help get your website found on search engines and social media&lt;/li>
&lt;li>&lt;strong>Media Galleries&lt;/strong> - Display your images and videos with captions in a customizable gallery&lt;/li>
&lt;li>&lt;strong>Mobile Friendly&lt;/strong> - Look amazing on every screen with a mobile friendly version of your site&lt;/li>
&lt;li>&lt;strong>Multi-language&lt;/strong> - 15+ language packs including English, 中文, and Português&lt;/li>
&lt;li>&lt;strong>Multi-user&lt;/strong> - Each author gets their own profile page&lt;/li>
&lt;li>&lt;strong>Privacy Pack&lt;/strong> - Assists with GDPR&lt;/li>
&lt;li>&lt;strong>Stand Out&lt;/strong> - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li>
&lt;li>&lt;strong>One-Click Deployment&lt;/strong> - No servers. No databases. Only files.&lt;/li>
&lt;/ul>
&lt;h2 id="themes">Themes&lt;/h2>
&lt;p>Academic comes with &lt;strong>automatic day (light) and night (dark) mode&lt;/strong> built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the
&lt;a href="https://academic-demo.netlify.com/" target="_blank" rel="noopener">Demo&lt;/a> to see it in action! Day/night mode can also be disabled by the site admin in &lt;code>params.toml&lt;/code>.&lt;/p>
&lt;p>
&lt;a href="https://sourcethemes.com/academic/themes/" target="_blank" rel="noopener">Choose a stunning &lt;strong>theme&lt;/strong> and &lt;strong>font&lt;/strong>&lt;/a> for your site. Themes are fully
&lt;a href="https://sourcethemes.com/academic/docs/customization/#custom-theme" target="_blank" rel="noopener">customizable&lt;/a>.&lt;/p>
&lt;h2 id="ecosystem">Ecosystem&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>
&lt;a href="https://github.com/sourcethemes/academic-admin" target="_blank" rel="noopener">Academic Admin&lt;/a>:&lt;/strong> An admin tool to import publications from BibTeX or import assets for an offline site&lt;/li>
&lt;li>&lt;strong>
&lt;a href="https://github.com/sourcethemes/academic-scripts" target="_blank" rel="noopener">Academic Scripts&lt;/a>:&lt;/strong> Scripts to help migrate content to new versions of Academic&lt;/li>
&lt;/ul>
&lt;h2 id="install">Install&lt;/h2>
&lt;p>You can choose from one of the following four methods to install:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://sourcethemes.com/academic/docs/install/#install-with-web-browser" target="_blank" rel="noopener">&lt;strong>one-click install using your web browser (recommended)&lt;/strong>&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://sourcethemes.com/academic/docs/install/#install-with-git" target="_blank" rel="noopener">install on your computer using &lt;strong>Git&lt;/strong> with the Command Prompt/Terminal app&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://sourcethemes.com/academic/docs/install/#install-with-zip" target="_blank" rel="noopener">install on your computer by downloading the &lt;strong>ZIP files&lt;/strong>&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://sourcethemes.com/academic/docs/install/#install-with-rstudio" target="_blank" rel="noopener">install on your computer with &lt;strong>RStudio&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Then
&lt;a href="https://sourcethemes.com/academic/docs/get-started/" target="_blank" rel="noopener">personalize and deploy your new site&lt;/a>.&lt;/p>
&lt;h2 id="updating">Updating&lt;/h2>
&lt;p>
&lt;a href="https://sourcethemes.com/academic/docs/update/" target="_blank" rel="noopener">View the Update Guide&lt;/a>.&lt;/p>
&lt;p>Feel free to &lt;em>star&lt;/em> the project on
&lt;a href="https://github.com/gcushen/hugo-academic/" target="_blank" rel="noopener">Github&lt;/a> to help keep track of
&lt;a href="https://sourcethemes.com/academic/updates" target="_blank" rel="noopener">updates&lt;/a>.&lt;/p>
&lt;h2 id="license">License&lt;/h2>
&lt;p>Copyright 2016-present
&lt;a href="https://georgecushen.com" target="_blank" rel="noopener">George Cushen&lt;/a>.&lt;/p>
&lt;p>Released under the
&lt;a href="https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md" target="_blank" rel="noopener">MIT&lt;/a> license.&lt;/p></description></item><item><title>Depth and Skeleton Associated Action Recognition without Online Accessible RGB-D Cameras</title><link>https://minhungchen.netlify.app/publication/mklda/</link><pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate><guid>https://minhungchen.netlify.app/publication/mklda/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Slides&lt;/em> button above to demo Academic&amp;rsquo;s Markdown slides feature.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -->
&lt;h2 id="resources">Resources&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;figure id="figure-paperhttpswwwcv-foundationorgopenaccesscontent_cvpr_2014paperslin_depth_and_skeleton_2014_cvpr_paperpdf">
&lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Lin_Depth_and_Skeleton_2014_CVPR_paper.pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;/a>
&lt;figcaption>
&lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Lin_Depth_and_Skeleton_2014_CVPR_paper.pdf">Paper&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-slidesslides_cvpr2014pdf">
&lt;a href="slides_CVPR2014.pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;/a>
&lt;figcaption>
&lt;a href="slides_CVPR2014.pdf">Slides&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;th>
&lt;figure id="figure-posterposter_cvpr2014pdf">
&lt;a href="poster_CVPR2014.pdf">
&lt;img src="https://minhungchen.netlify.app/img/pdf_icon.png" alt="" width="40%" >
&lt;/a>
&lt;figcaption>
&lt;a href="poster_CVPR2014.pdf">Poster&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table>
&lt;p>Other Links:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Lin_Depth_and_Skeleton_2014_CVPR_paper.html" target="_blank" rel="noopener">CVF Open Access&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://ieeexplore.ieee.org/document/6909731" target="_blank" rel="noopener">IEEE Xplore&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;p>
&lt;a href="https://sites.google.com/site/yylinweb/" target="_blank" rel="noopener">Yen-Yu Lin&lt;/a>,
&lt;a href="https://www.linkedin.com/in/juhsuanhua/" target="_blank" rel="noopener">Ju-Hsuan Hua&lt;/a>,
&lt;a href="https://www.linkedin.com/in/nick-tang-18702933/" target="_blank" rel="noopener">Nick C. Tang&lt;/a>,
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">&lt;strong>Min-Hung Chen&lt;/strong>&lt;/a>, and
&lt;a href="https://scholar.google.com/citations?user=_IXt8boAAAAJ" target="_blank" rel="noopener">Hong-Yuan (Mark) Liao&lt;/a>, &amp;ldquo;Depth and Skeleton Associated Action Recognition without Online Accessible RGB-D Cameras&amp;rdquo;,
&lt;a href="http://www.pamitc.org/cvpr14/" target="_blank" rel="noopener">&lt;em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em>, 2014&lt;/a>.&lt;/p>
&lt;h4 id="bibtex">BibTex&lt;/h4>
&lt;pre>&lt;code>@inproceedings{lin2014depth,
title={Depth and skeleton associated action recognition without online accessible rgb-d cameras},
author={Lin, Yen-Yu and Hua, Ju-Hsuan and Tang, Nick C and Chen, Min-Hung and Mark Liao, Hong-Yuan},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2014}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="members">Members&lt;/h2>
&lt;p>&lt;strong>Academia Sinica, Taiwan &lt;br>&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>
&lt;a href="https://sites.google.com/site/yylinweb/" target="_blank" rel="noopener">
&lt;figure id="figure-yen-yu-lin">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_yyl.jpg" alt="" width="100%" >
&lt;figcaption>
Yen-Yu Lin
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/juhsuanhua/" target="_blank" rel="noopener">
&lt;figure id="figure-ju-hsuan-hua">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_jhh.jpg" alt="" width="100%" >
&lt;figcaption>
Ju-Hsuan Hua
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://www.linkedin.com/in/nick-tang-18702933/" target="_blank" rel="noopener">
&lt;figure id="figure-nick-tang">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_nt.jpg" alt="" width="100%" >
&lt;figcaption>
Nick Tang
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://minhungchen.netlify.app/" target="_blank" rel="noopener">
&lt;figure id="figure-min-hung-chen">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_me.jpg" alt="" width="100%" >
&lt;figcaption>
Min-Hung Chen
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;th>
&lt;a href="https://scholar.google.com/citations?user=_IXt8boAAAAJ" target="_blank" rel="noopener">
&lt;figure id="figure-mark-liao">
&lt;img src="https://minhungchen.netlify.app/img/authors/head_hyl.jpg" alt="" width="100%" >
&lt;figcaption>
Mark Liao
&lt;/figcaption>
&lt;/figure>
&lt;/a>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;/table></description></item></channel></rss>