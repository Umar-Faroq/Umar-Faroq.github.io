<!doctype html><html lang=en-us>
<!-- Mirrored from minhungchen.netlify.app/publication/ta3n/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 04 Mar 2022 06:29:12 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Min-Hung Chen"><meta name=description content="\[**ICCV 2019 (Oral)**\] Cross-domain action recognition with new datasets and novel attention-based DA approaches."><link rel=alternate hreflang=en-us href=index.html><meta name=theme-color content="#2962ff"><script src=../../js/mathjax-config.js></script><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=../../../cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><script src=../../../cdn.jsdelivr.net/npm/mathjax%403/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&amp;display=swap"><link rel=stylesheet href=../../css/academic.min.ed35f6de5da3623407f7505649fd9d16.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-168846389-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
function trackOutboundLink(url,target){gtag('event','click',{'event_category':'outbound','event_label':url,'transport_type':'beacon','event_callback':function(){if(target!=='_blank'){document.location=url;}}});console.debug("Outbound link clicked: "+url);}
function onClickCallback(event){if((event.target.tagName!=='A')||(event.target.host===window.location.host)){return;}
trackOutboundLink(event.target,event.target.getAttribute('target'));}
gtag('js',new Date());gtag('config','UA-168846389-1',{});document.addEventListener('click',onClickCallback,false);</script><link rel=manifest href=../../index.webmanifest><link rel=icon type=image/png href=../../images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=../../images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png><link rel=canonical href=index.html><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@CMHungSteven"><meta property="twitter:creator" content="@CMHungSteven"><meta property="og:site_name" content="Min-Hung Chen"><meta property="og:url" content="https://minhungchen.netlify.app/publication/ta3n/"><meta property="og:title" content="Temporal Attentive Alignment for Large-Scale Video Domain Adaptation | Min-Hung Chen"><meta property="og:description" content="\[**ICCV 2019 (Oral)**\] Cross-domain action recognition with new datasets and novel attention-based DA approaches."><meta property="og:image" content="https://minhungchen.netlify.app/publication/ta3n/featured.png"><meta property="twitter:image" content="https://minhungchen.netlify.app/publication/ta3n/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-05-16T00:00:00+00:00"><meta property="article:modified_time" content="2021-04-25T12:24:53+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://minhungchen.netlify.app/publication/ta3n/"},"headline":"Temporal Attentive Alignment for Large-Scale Video Domain Adaptation","image":["https://minhungchen.netlify.app/publication/ta3n/featured.png"],"datePublished":"2020-05-16T00:00:00Z","dateModified":"2021-04-25T12:24:53+08:00","author":{"@type":"Person","name":"Min-Hung Chen"},"publisher":{"@type":"Organization","name":"Min-Hung Chen","logo":{"@type":"ImageObject","url":"https://minhungchen.netlify.app/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"}},"description":"\\[**ICCV 2019 (Oral)**\\] Cross-domain action recognition with new datasets and novel attention-based DA approaches."}</script><title>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation | Min-Hung Chen</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=../../index.html>Min-Hung Chen</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=../../index.html>Min-Hung Chen</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=../../index.html#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#news><span>News</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#activities><span>Activities</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#honors><span>Honors</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#teaching><span>Teaching</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class="nav-link js-theme-selector" data-toggle=dropdown aria-haspopup=true><i class="fas fa-palette" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav><div class=pub><div class="article-container pt-3"><h1>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</h1><div class=article-metadata><div><span><a href=../../author/min-hung-chen/index.html>Min-Hung Chen</a></span>, <span>Zsolt Kira</span>, <span>Ghassan AlRegib</span>, <span>Jaekwon Yoo</span>, <span>Ruxin Chen</span>, <span>Jian Zheng</span></div><span class=article-date>October 2019</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=https://arxiv.org/pdf/1907.12743.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 js-cite-modal" data-filename=/publication/ta3n/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1" href=https://github.com/cmhungsteve/TA3N target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://github.com/cmhungsteve/TA3N#dataset-preparation target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary my-1 mr-1" href=../../project/cdar/index.html>Project</a>
<a class="btn btn-outline-primary my-1 mr-1" href="https://www.dropbox.com/s/nsb9bdfaz03mjv2/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0" target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary my-1 mr-1" href="https://www.dropbox.com/s/7p1u8yro9hqseac/Oral_TA3N_ICCV_2019_mute.pdf?dl=0" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://doi.org/10.1109/ICCV.2019.00642 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://arxiv.org/abs/1907.12743 target=_blank rel=noopener>ArXiv</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://mlatgt.blog/2019/09/10/overcoming-large-scale-annotation-requirements-for-understanding-videos-in-the-wild target=_blank rel=noopener>Blog</a>
<a class="btn btn-outline-primary my-1 mr-1" href=../../talk/TA3N/index.html>Talk</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://youtu.be/j9cDuzmpYP8 target=_blank rel=noopener>Oral Video</a>
<a class="btn btn-outline-primary my-1 mr-1" href=http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.html target=_blank rel=noopener>CVF Open Access</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://ieeexplore.ieee.org/document/9008391 target=_blank rel=noopener>IEEE Xplore</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:643px;max-height:557px><div style=position:relative><img src=featured.png alt class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose two large-scale video DA datasets with much larger domain discrepancy, <strong>UCF-HMDB<sub>full</sub></strong> and <strong>Kinetics-Gameplay</strong>. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose <strong>Temporal Attentive Adversarial Adaptation Network (TA<sup>3</sup>N)</strong>, which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on four video DA datasets (e.g. 7.9% accuracy gain over &ldquo;Source only&rdquo; from 73.9% to 81.8% on &ldquo;HMDB&ndash;>UCF&rdquo;, and 10.3% gain on &ldquo;Kinetics&ndash;>Gameplay&rdquo;).</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=../index.html#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">In <em>International Conference on Computer Vision (ICCV)</em> <strong>[Oral (4.6% acceptance rate)]</strong></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><h2 id=videos>Videos</h2><p>Oral Video (please turn on closed captions):<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/j9cDuzmpYP8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><hr><h2 id=resources>Resources</h2><figure id=figure-codehttpsgithubcomcmhungsteveta3n><a href=https://github.com/cmhungsteve/TA3N><img src=../../img/github_icon.png alt width=15%></a><figcaption><a href=https://github.com/cmhungsteve/TA3N><strong>Code</strong></a></figcaption></figure><table><thead><tr><th><figure id=figure-paperhttpsarxivorgpdf190712743pdf><a href=https://arxiv.org/pdf/1907.12743.pdf><img src=../../img/pdf_icon.png alt width=40%></a><figcaption><a href=https://arxiv.org/pdf/1907.12743.pdf>Paper</a></figcaption></figure></th><th><figure id=figure-slideshttpswwwdropboxcomss9ud77a1zt0vqbnoral_ta3n_iccv_2019_mutepdfdl0><a href="https://www.dropbox.com/s/s9ud77a1zt0vqbn/Oral_TA3N_ICCV_2019_mute.pdf?dl=0"><img src=../../img/pdf_icon.png alt width=40%></a><figcaption><a href="https://www.dropbox.com/s/s9ud77a1zt0vqbn/Oral_TA3N_ICCV_2019_mute.pdf?dl=0">Slides</a></figcaption></figure></th><th><figure id=figure-posterhttpswwwdropboxcomsg16v02k9sgn1xibiccv2019_steve_ta3n_poster_v1_2pdfdl0><a href="https://www.dropbox.com/s/g16v02k9sgn1xib/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0"><img src=../../img/pdf_icon.png alt width=40%></a><figcaption><a href="https://www.dropbox.com/s/g16v02k9sgn1xib/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0">Poster</a></figcaption></figure></th></tr></thead></table><p>Other Links:</p><ul><li><a href=http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.html target=_blank rel=noopener>CVF Open Access</a></li><li><a href=https://ieeexplore.ieee.org/document/9008391 target=_blank rel=noopener>IEEE Xplore</a></li><li><a href=https://mlatgt.blog/2019/09/10/overcoming-large-scale-annotation-requirements-for-understanding-videos-in-the-wild target=_blank rel=noopener>ML@GT Blog</a></li><li><a href=https://mailchi.mp/4ed0cbf6a67d/iccv2019 target=_blank rel=noopener>GT@ICCV'19</a></li><li>CVPR workshop on Learning from Unlabeled Videos (LUV) [
<a href=https://sites.google.com/view/luv2019/program target=_blank rel=noopener>CVPRW'19</a> ]</li></ul><hr><h2 id=citation>Citation</h2><ul><li><a href=../../index.html target=_blank rel=noopener><strong>Min-Hung Chen</strong></a>,
<a href=https://www.cc.gatech.edu/~zk15/ target=_blank rel=noopener>Zsolt Kira</a>,
<a href=https://ghassanalregib.info/ target=_blank rel=noopener>Ghassan AlRegib</a>,
<a href=https://www.linkedin.com/in/jaekwon-yoo-8685862b/ target=_blank rel=noopener>Jaekwon Yoo</a>,
<a href=(https_/www.linkedin.com/in/ruxin-chen-991477119/).html>Ruxin Chen</a>, and
<a href="https://scholar.google.com/citations?user=5YR6dTEAAAAJ" target=_blank rel=noopener>Jian Zheng</a>, &ldquo;Temporal Attentive Alignment for Large-Scale Video Domain Adaptation&rdquo;,
<a href=http://iccv2019.thecvf.com/ target=_blank rel=noopener><em>IEEE International Conference on Computer Vision (ICCV)</em>, 2019</a> <strong>[Oral (acceptance rate: 4.6%), travel grant awarded]</strong>.</li><li><a href=../../index.html target=_blank rel=noopener><strong>Min-Hung Chen</strong></a>,
<a href=https://www.cc.gatech.edu/~zk15/ target=_blank rel=noopener>Zsolt Kira</a>, and
<a href=https://ghassanalregib.info/ target=_blank rel=noopener>Ghassan AlRegib</a>, &ldquo;Temporal Attentive Alignment for Video Domain Adaptation&rdquo;,
<a href=https://sites.google.com/view/luv2019 target=_blank rel=noopener><em>CVPR Workshop on Learning from Unlabeled Videos (LUV)</em>, 2019</a>.</li></ul><h4 id=bibtex>BibTex</h4><pre><code>@inproceedings{chen2019temporal,
  title={Temporal attentive alignment for large-scale video domain adaptation},
  author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan and Yoo, Jaekwon and Chen, Ruxin and Zheng, Jian},
  booktitle={IEEE International Conference on Computer Vision (ICCV)},
  year={2019}
}

@article{chen2019taaan,
  title={Temporal Attentive Alignment for Video Domain Adaptation},
  author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan},
  journal={CVPR Workshop on Learning from Unlabeled Videos},
  year={2019},
  url={https://arxiv.org/abs/1905.10861}
}
</code></pre><hr><h2 id=members>Members</h2><p><strong><sup>1</sup>Georgia Institute of Technology   <sup>2</sup>Sony Interactive Entertainment LLC   <sup>3</sup>Binghamton University<br></strong>*work partially done as a SIE intern</p><table><thead><tr><th><a href=../../index.html target=_blank rel=noopener><figure id=figure-min-hung-chensup1sup><img src=../../img/authors/head_me.jpg alt width=100%><figcaption>Min-Hung Chen<sup>1</sup>*</figcaption></figure></a></th><th><a href=https://www.cc.gatech.edu/~zk15/ target=_blank rel=noopener><figure id=figure-zsolt-kirasup1sup><img src=../../img/authors/head_zk.jpg alt width=100%><figcaption>Zsolt Kira<sup>1</sup></figcaption></figure></a></th><th><a href=https://ghassanalregib.info/ target=_blank rel=noopener><figure id=figure-ghassan-alregibsup1sup><img src=../../img/authors/head_ga.jpg alt width=100%><figcaption>Ghassan AlRegib<sup>1</sup></figcaption></figure></a></th><th><a href=https://www.linkedin.com/in/jaekwon-yoo-8685862b/ target=_blank rel=noopener><figure id=figure-jaekwon-yoosup2sup><img src=../../img/authors/head_unknown.jpg alt width=100%><figcaption>Jaekwon Yoo<sup>2</sup></figcaption></figure></a></th><th><a href=https://www.linkedin.com/in/ruxin-chen-991477119/ target=_blank rel=noopener><figure id=figure-ruxin-chensup2sup><img src=../../img/authors/head_unknown.jpg alt width=100%><figcaption>Ruxin Chen<sup>2</sup></figcaption></figure></a></th><th><a href="https://scholar.google.com/citations?user=5YR6dTEAAAAJ" target=_blank rel=noopener><figure id=figure-jian-zhengsup3sup><img src=../../img/authors/head_jz.jpg alt width=100%><figcaption>Jian Zheng<sup>3</sup>*</figcaption></figure></a></th></tr></thead></table></div><div class=article-tags><a class="badge badge-light" href=../../tag/action-recognition/index.html>Action Recognition</a>
<a class="badge badge-light" href=../../tag/datasets/index.html>Datasets</a>
<a class="badge badge-light" href=../../tag/domain-adaptation/index.html>Domain Adaptation</a>
<a class="badge badge-light" href=../../tag/iccv/index.html>ICCV</a>
<a class="badge badge-light" href=../../tag/oral/index.html>Oral</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://minhungchen.netlify.app/publication/ta3n/&amp;text=Temporal%20Attentive%20Alignment%20for%20Large-Scale%20Video%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://minhungchen.netlify.app/publication/ta3n/&amp;t=Temporal%20Attentive%20Alignment%20for%20Large-Scale%20Video%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Temporal%20Attentive%20Alignment%20for%20Large-Scale%20Video%20Domain%20Adaptation&body=https://minhungchen.netlify.app/publication/ta3n/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://minhungchen.netlify.app/publication/ta3n/&amp;title=Temporal%20Attentive%20Alignment%20for%20Large-Scale%20Video%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Temporal%20Attentive%20Alignment%20for%20Large-Scale%20Video%20Domain%20Adaptation%20https://minhungchen.netlify.app/publication/ta3n/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://minhungchen.netlify.app/publication/ta3n/&amp;title=Temporal%20Attentive%20Alignment%20for%20Large-Scale%20Video%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=../../author/min-hung-chen/avatar_huc3acf39b4c2265ec309e042ecb270b7c_902720_270x270_fill_lanczos_center_2.png alt="Min-Hung Chen"><div class=media-body><h5 class=card-title><a href=../../index.html>Min-Hung Chen</a></h5><h6 class=card-subtitle>Research Engineer II</h6><p class=card-text>My research interest is Learning without Fully Supervision.</p><ul class=network-icon aria-hidden=true><li><a href=../../index.html#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/chensteven target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://github.com/cmhungsteve target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://twitter.com/CMHungSteven target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=ovzuxi8AAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=../../files/cv.pdf><i class="ai ai-cv"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=../tslstm/index.html>TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition</a></li></ul></div></div></div><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/python.min.js></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=../../../cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=../../js/academic.min.37431be2d92d7fb0160054761ab79602.js></script><div class=container><footer class=site-footer><p class=powered-by>© 2020 Min-Hung Chen</p><p class=powered-by>Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body>
<!-- Mirrored from minhungchen.netlify.app/publication/ta3n/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 04 Mar 2022 06:29:14 GMT -->
</html>