<!doctype html><html lang=en-us>
<!-- Mirrored from minhungchen.netlify.app/publication/mklda/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 04 Mar 2022 06:29:25 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Min-Hung Chen"><meta name=description content="\[**CVPR 2014**\] Multi-modal adaptation with multiple kernel learning for action recognition."><link rel=alternate hreflang=en-us href=index.html><meta name=theme-color content="#2962ff"><script src=../../js/mathjax-config.js></script><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=../../../cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><script src=../../../cdn.jsdelivr.net/npm/mathjax%403/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&amp;display=swap"><link rel=stylesheet href=../../css/academic.min.ed35f6de5da3623407f7505649fd9d16.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-168846389-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
function trackOutboundLink(url,target){gtag('event','click',{'event_category':'outbound','event_label':url,'transport_type':'beacon','event_callback':function(){if(target!=='_blank'){document.location=url;}}});console.debug("Outbound link clicked: "+url);}
function onClickCallback(event){if((event.target.tagName!=='A')||(event.target.host===window.location.host)){return;}
trackOutboundLink(event.target,event.target.getAttribute('target'));}
gtag('js',new Date());gtag('config','UA-168846389-1',{});document.addEventListener('click',onClickCallback,false);</script><link rel=manifest href=../../index.webmanifest><link rel=icon type=image/png href=../../images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=../../images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png><link rel=canonical href=index.html><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@CMHungSteven"><meta property="twitter:creator" content="@CMHungSteven"><meta property="og:site_name" content="Min-Hung Chen"><meta property="og:url" content="https://minhungchen.netlify.app/publication/mklda/"><meta property="og:title" content="Depth and Skeleton Associated Action Recognition without Online Accessible RGB-D Cameras | Min-Hung Chen"><meta property="og:description" content="\[**CVPR 2014**\] Multi-modal adaptation with multiple kernel learning for action recognition."><meta property="og:image" content="https://minhungchen.netlify.app/publication/mklda/featured.png"><meta property="twitter:image" content="https://minhungchen.netlify.app/publication/mklda/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-05-30T00:00:00+00:00"><meta property="article:modified_time" content="2021-02-20T08:10:37+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://minhungchen.netlify.app/publication/mklda/"},"headline":"Depth and Skeleton Associated Action Recognition without Online Accessible RGB-D Cameras","image":["https://minhungchen.netlify.app/publication/mklda/featured.png"],"datePublished":"2020-05-30T00:00:00Z","dateModified":"2021-02-20T08:10:37+08:00","author":{"@type":"Person","name":"Yen-Yu Lin"},"publisher":{"@type":"Organization","name":"Min-Hung Chen","logo":{"@type":"ImageObject","url":"https://minhungchen.netlify.app/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"}},"description":"\\[**CVPR 2014**\\] Multi-modal adaptation with multiple kernel learning for action recognition."}</script><title>Depth and Skeleton Associated Action Recognition without Online Accessible RGB-D Cameras | Min-Hung Chen</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=../../index.html>Min-Hung Chen</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=../../index.html>Min-Hung Chen</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=../../index.html#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#news><span>News</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#activities><span>Activities</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#honors><span>Honors</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#teaching><span>Teaching</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class="nav-link js-theme-selector" data-toggle=dropdown aria-haspopup=true><i class="fas fa-palette" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav><div class=pub><div class="article-container pt-3"><h1>Depth and Skeleton Associated Action Recognition without Online Accessible RGB-D Cameras</h1><div class=article-metadata><div><span>Yen-Yu Lin</span>, <span>Ju-Hsuan Hua</span>, <span>Nick C. Tang</span>, <span><a href=../../author/min-hung-chen/index.html>Min-Hung Chen</a></span>, <span>Hong-Yuan (Mark) Liao</span></div><span class=article-date>June 2014</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Lin_Depth_and_Skeleton_2014_CVPR_paper.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 js-cite-modal" data-filename=/publication/mklda/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1" href=poster_CVPR2014.pdf target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary my-1 mr-1" href=slides_CVPR2014.pdf target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://doi.org/10.1109/CVPR.2014.335 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Lin_Depth_and_Skeleton_2014_CVPR_paper.html target=_blank rel=noopener>CVF Open Access</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://ieeexplore.ieee.org/document/6909731 target=_blank rel=noopener>IEEE Xplore</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:425px><div style=position:relative><img src=featured_hu332bc8582dfe5d19b62f760b8e26ed38_382594_720x0_resize_lanczos_2.png alt class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>The recent advances in RGB-D cameras have allowed us to better solve increasingly complex computer vision tasks. However, modern RGB-D cameras are still restricted by the short effective distances. The limitation may make RGB-D cameras not online accessible in practice, and degrade their applicability. We propose an alternative scenario to address this problem, and illustrate it with the application to action recognition. We use Kinect to offline collect an auxiliary, multi-modal database, in which not only the RGB videos but also the depth maps and skeleton structures of actions of interest are available. Our approach aims to enhance action recognition in RGB videos by leveraging the extra database. Specifically, it optimizes a feature transformation, by which the actions to be recognized can be concisely reconstructed by entries in the auxiliary database. In this way, the inter-database variations are adapted. More importantly, each action can be augmented with additional depth and skeleton images retrieved from the auxiliary database. The proposed approach has been evaluated on three benchmarks of action recognition. The promising results manifest that the augmented depth and skeleton features can lead to remarkable boost in recognition accuracy.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=../index.html#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">In <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><h2 id=resources>Resources</h2><table><thead><tr><th><figure id=figure-paperhttpswwwcv-foundationorgopenaccesscontent_cvpr_2014paperslin_depth_and_skeleton_2014_cvpr_paperpdf><a href=https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Lin_Depth_and_Skeleton_2014_CVPR_paper.pdf><img src=../../img/pdf_icon.png alt width=40%></a><figcaption><a href=https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Lin_Depth_and_Skeleton_2014_CVPR_paper.pdf>Paper</a></figcaption></figure></th><th><figure id=figure-slidesslides_cvpr2014pdf><a href=slides_CVPR2014.pdf><img src=../../img/pdf_icon.png alt width=40%></a><figcaption><a href=slides_CVPR2014.pdf>Slides</a></figcaption></figure></th><th><figure id=figure-posterposter_cvpr2014pdf><a href=poster_CVPR2014.pdf><img src=../../img/pdf_icon.png alt width=40%></a><figcaption><a href=poster_CVPR2014.pdf>Poster</a></figcaption></figure></th></tr></thead></table><p>Other Links:</p><ul><li><a href=https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Lin_Depth_and_Skeleton_2014_CVPR_paper.html target=_blank rel=noopener>CVF Open Access</a></li><li><a href=https://ieeexplore.ieee.org/document/6909731 target=_blank rel=noopener>IEEE Xplore</a></li></ul><hr><h2 id=citation>Citation</h2><p><a href=https://sites.google.com/site/yylinweb/ target=_blank rel=noopener>Yen-Yu Lin</a>,
<a href=https://www.linkedin.com/in/juhsuanhua/ target=_blank rel=noopener>Ju-Hsuan Hua</a>,
<a href=https://www.linkedin.com/in/nick-tang-18702933/ target=_blank rel=noopener>Nick C. Tang</a>,
<a href=../../index.html target=_blank rel=noopener><strong>Min-Hung Chen</strong></a>, and
<a href="https://scholar.google.com/citations?user=_IXt8boAAAAJ" target=_blank rel=noopener>Hong-Yuan (Mark) Liao</a>, &ldquo;Depth and Skeleton Associated Action Recognition without Online Accessible RGB-D Cameras&rdquo;,
<a href=http://www.pamitc.org/cvpr14/ target=_blank rel=noopener><em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2014</a>.</p><h4 id=bibtex>BibTex</h4><pre><code>@inproceedings{lin2014depth,
  title={Depth and skeleton associated action recognition without online accessible rgb-d cameras},
  author={Lin, Yen-Yu and Hua, Ju-Hsuan and Tang, Nick C and Chen, Min-Hung and Mark Liao, Hong-Yuan},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2014}
}
</code></pre><hr><h2 id=members>Members</h2><p><strong>Academia Sinica, Taiwan<br></strong></p><table><thead><tr><th><a href=https://sites.google.com/site/yylinweb/ target=_blank rel=noopener><figure id=figure-yen-yu-lin><img src=../../img/authors/head_yyl.jpg alt width=100%><figcaption>Yen-Yu Lin</figcaption></figure></a></th><th><a href=https://www.linkedin.com/in/juhsuanhua/ target=_blank rel=noopener><figure id=figure-ju-hsuan-hua><img src=../../img/authors/head_jhh.jpg alt width=100%><figcaption>Ju-Hsuan Hua</figcaption></figure></a></th><th><a href=https://www.linkedin.com/in/nick-tang-18702933/ target=_blank rel=noopener><figure id=figure-nick-tang><img src=../../img/authors/head_nt.jpg alt width=100%><figcaption>Nick Tang</figcaption></figure></a></th><th><a href=../../index.html target=_blank rel=noopener><figure id=figure-min-hung-chen><img src=../../img/authors/head_me.jpg alt width=100%><figcaption>Min-Hung Chen</figcaption></figure></a></th><th><a href="https://scholar.google.com/citations?user=_IXt8boAAAAJ" target=_blank rel=noopener><figure id=figure-mark-liao><img src=../../img/authors/head_hyl.jpg alt width=100%><figcaption>Mark Liao</figcaption></figure></a></th></tr></thead></table></div><div class=article-tags><a class="badge badge-light" href=../../tag/action-recognition/index.html>Action Recognition</a>
<a class="badge badge-light" href=../../tag/cvpr/index.html>CVPR</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://minhungchen.netlify.app/publication/mklda/&amp;text=Depth%20and%20Skeleton%20Associated%20Action%20Recognition%20without%20Online%20Accessible%20RGB-D%20Cameras" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://minhungchen.netlify.app/publication/mklda/&amp;t=Depth%20and%20Skeleton%20Associated%20Action%20Recognition%20without%20Online%20Accessible%20RGB-D%20Cameras" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Depth%20and%20Skeleton%20Associated%20Action%20Recognition%20without%20Online%20Accessible%20RGB-D%20Cameras&body=https://minhungchen.netlify.app/publication/mklda/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://minhungchen.netlify.app/publication/mklda/&amp;title=Depth%20and%20Skeleton%20Associated%20Action%20Recognition%20without%20Online%20Accessible%20RGB-D%20Cameras" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Depth%20and%20Skeleton%20Associated%20Action%20Recognition%20without%20Online%20Accessible%20RGB-D%20Cameras%20https://minhungchen.netlify.app/publication/mklda/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://minhungchen.netlify.app/publication/mklda/&amp;title=Depth%20and%20Skeleton%20Associated%20Action%20Recognition%20without%20Online%20Accessible%20RGB-D%20Cameras" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=../../author/min-hung-chen/avatar_huc3acf39b4c2265ec309e042ecb270b7c_902720_270x270_fill_lanczos_center_2.png alt="Min-Hung Chen"><div class=media-body><h5 class=card-title><a href=../../index.html>Min-Hung Chen</a></h5><h6 class=card-subtitle>Research Engineer II</h6><p class=card-text>My research interest is Learning without Fully Supervision.</p><ul class=network-icon aria-hidden=true><li><a href=../../index.html#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/chensteven target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://github.com/cmhungsteve target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://twitter.com/CMHungSteven target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=ovzuxi8AAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=../../files/cv.pdf><i class="ai ai-cv"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=../sstda/index.html>Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation</a></li><li><a href=../tslstm/index.html>TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition</a></li><li><a href=../ta3n/index.html>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</a></li></ul></div></div></div><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/python.min.js></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=../../../cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=../../js/academic.min.37431be2d92d7fb0160054761ab79602.js></script><div class=container><footer class=site-footer><p class=powered-by>Â© 2020 Min-Hung Chen</p><p class=powered-by>Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body>
<!-- Mirrored from minhungchen.netlify.app/publication/mklda/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 04 Mar 2022 06:29:29 GMT -->
</html>