<!doctype html><html lang=en-us>
<!-- Mirrored from minhungchen.netlify.app/project/cdas/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 04 Mar 2022 06:28:38 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Min-Hung Chen"><meta name=description content="Cross-domain action segmentation by aligning temporal feature spaces."><link rel=alternate hreflang=en-us href=index.html><meta name=theme-color content="#2962ff"><script src=../../js/mathjax-config.js></script><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=../../../cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><script src=../../../cdn.jsdelivr.net/npm/mathjax%403/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&amp;display=swap"><link rel=stylesheet href=../../css/academic.min.ed35f6de5da3623407f7505649fd9d16.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-168846389-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
function trackOutboundLink(url,target){gtag('event','click',{'event_category':'outbound','event_label':url,'transport_type':'beacon','event_callback':function(){if(target!=='_blank'){document.location=url;}}});console.debug("Outbound link clicked: "+url);}
function onClickCallback(event){if((event.target.tagName!=='A')||(event.target.host===window.location.host)){return;}
trackOutboundLink(event.target,event.target.getAttribute('target'));}
gtag('js',new Date());gtag('config','UA-168846389-1',{});document.addEventListener('click',onClickCallback,false);</script><link rel=manifest href=../../index.webmanifest><link rel=icon type=image/png href=../../images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=../../images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png><link rel=canonical href=index.html><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@CMHungSteven"><meta property="twitter:creator" content="@CMHungSteven"><meta property="og:site_name" content="Min-Hung Chen"><meta property="og:url" content="https://minhungchen.netlify.app/project/cdas/"><meta property="og:title" content="Action Segmentation with Temporal Domain Adaptation | Min-Hung Chen"><meta property="og:description" content="Cross-domain action segmentation by aligning temporal feature spaces."><meta property="og:image" content="https://minhungchen.netlify.app/project/cdas/featured.png"><meta property="twitter:image" content="https://minhungchen.netlify.app/project/cdas/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-06-25T00:00:00+00:00"><meta property="article:modified_time" content="2021-02-20T08:10:37+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://minhungchen.netlify.app/project/cdas/"},"headline":"Action Segmentation with Temporal Domain Adaptation","image":["https://minhungchen.netlify.app/project/cdas/featured.png"],"datePublished":"2020-06-25T00:00:00Z","dateModified":"2021-02-20T08:10:37+08:00","author":{"@type":"Person","name":"Min-Hung Chen"},"publisher":{"@type":"Organization","name":"Min-Hung Chen","logo":{"@type":"ImageObject","url":"https://minhungchen.netlify.app/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"}},"description":"Cross-domain action segmentation by aligning temporal feature spaces."}</script><title>Action Segmentation with Temporal Domain Adaptation | Min-Hung Chen</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=../../index.html>Min-Hung Chen</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=../../index.html>Min-Hung Chen</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=../../index.html#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#news><span>News</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#activities><span>Activities</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#honors><span>Honors</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#teaching><span>Teaching</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class="nav-link js-theme-selector" data-toggle=dropdown aria-haspopup=true><i class="fas fa-palette" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav><article class="article article-project"><div class="article-container pt-3"><h1>Action Segmentation with Temporal Domain Adaptation</h1><div class=article-metadata><div><span><a href=../../author/min-hung-chen/index.html>Min-Hung Chen</a></span>, <span>Baopu Li</span>, <span>Yingze Bao</span>, <span>Ghassan AlRegib</span>, <span>Zsolt Kira</span></div><span class=article-date>Last updated on
Feb 20, 2021</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=../../publication/SSTDA/index-2.html>CVPR'20</a>
<a class="btn btn-outline-primary my-1 mr-1" href=../../publication/MTDA/index-2.html>WACV'20</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:372px><div style=position:relative><img src=featured_hu82a37364f0115129e2528466f011bbda_436309_720x0_resize_lanczos_2.png alt class=featured-image></div></div><div class=article-container><div class=article-style><h2 id=motivation>Motivation</h2><p>Despite the recent progress of fully-supervised action segmentation techniques, the performance is still not fully satisfactory.
Exploiting larger-scale labeled data and designing more complicated architectures result in additional annotation and computation costs.
Therefore, we aim to exploit auxiliary unlabeled videos, which are
comparatively easy to obtain, to improve the performance.</p><hr><h2 id=challenges>Challenges</h2><p>One main challenge of utilizing unlabeled videos is the problem of <strong>spatio-temporal variations</strong>. For example, different people may <em>make tea</em> in different personalized styles even if the given recipe is the same. The intra-class variations cause degraded performance by directly deploying a model trained with different groups of people.</p><hr><h2 id=our-approaches>Our Approaches</h2><p>We exploit unlabeled videos to address this problem by reformulating the action segmentation task as a cross-domain problem with domain discrepancy caused by spatio-temporal variations.<br>To reduce the discrepancy, we propose two approaches:</p><ul><li><strong>Mixed Temporal Domain Adaptation (MTDA)</strong>: align the features embedded with local and global temporal dynamics.<figure><a data-fancybox href=approach_MTDA_hucffefdbeb1823fdb487e1b3e7c55d0f8_535391_2000x2000_fit_lanczos_2.png><img data-src=/project/cdas/approach_MTDA_hucffefdbeb1823fdb487e1b3e7c55d0f8_535391_2000x2000_fit_lanczos_2.png class=lazyload alt width=70% height=739></a></figure></li><li><strong>Self-Supervised Temporal Domain Adaptation (SSTDA)</strong>: align the feature spaces across multiple temporal scales with self-supervised learning.<figure><a data-fancybox href=approach_SSTDA_hu576d5af5b9c05786c04595278edfc0cf_152205_2000x2000_fit_lanczos_2.png><img data-src=/project/cdas/approach_SSTDA_hu576d5af5b9c05786c04595278edfc0cf_152205_2000x2000_fit_lanczos_2.png class=lazyload alt width=100% height=624></a></figure></li></ul><hr><h2 id=results>Results</h2><p>We evaluate three challenging benchmark datasets: <strong>GTEA</strong>, <strong>50Salads</strong>, and <strong>Breakfast</strong>, and achieve the follows:</p><ul><li>Outperform other Domain Adaptation (DA) and video-based self-supervised approaches.<figure id=figure-the-comparison-of-different-methods-that-can-learn-information-from-unlabeled-target-videos-on-gtea><a data-fancybox href=result_SSTDA_DA_hu461714772ec65b7d7fa817e5e0e7d2dd_84503_2000x2000_fit_lanczos_2.png data-caption="The comparison of different methods that can learn information from unlabeled target videos (on GTEA)."><img data-src=/project/cdas/result_SSTDA_DA_hu461714772ec65b7d7fa817e5e0e7d2dd_84503_2000x2000_fit_lanczos_2.png class=lazyload alt width=50% height=361></a><figcaption>The comparison of different methods that can learn information from unlabeled target videos (on GTEA).</figcaption></figure></li><li>Outperform the current state-of-the-art action segmentation methods by large margins.</li><li>Achieve comparable performance with fully-supervised methods using only 65% of the labeled training data.<figure id=figure-comparison-with-the-most-recent-action-segmentation-methods-on-all-three-datasets><a data-fancybox href=result_SSTDA_SOTA_hu47e5dce5026a3482fb93f68f57e4d656_162875_2000x2000_fit_lanczos_2.png data-caption="Comparison with the most recent action segmentation methods on all three datasets."><img data-src=/project/cdas/result_SSTDA_SOTA_hu47e5dce5026a3482fb93f68f57e4d656_162875_2000x2000_fit_lanczos_2.png class=lazyload alt width=50% height=639></a><figcaption>Comparison with the most recent action segmentation methods on all three datasets.</figcaption></figure><figure id=figure--the-visualization-of-temporal-action-segmentation-for-our-methods-with-color-coding-input-example-make-coffee><a data-fancybox href=result_SSTDA_visualization_hude82285d87074eb140b18b35b3cd7515_99399_2000x2000_fit_lanczos_2.png data-caption="The visualization of temporal action segmentation for our methods with color-coding (input example: make coffee)"><img data-src=/project/cdas/result_SSTDA_visualization_hude82285d87074eb140b18b35b3cd7515_99399_2000x2000_fit_lanczos_2.png class=lazyload alt width=100% height=310></a><figcaption>The visualization of temporal action segmentation for our methods with color-coding (input example: make coffee)</figcaption></figure></li></ul><p>Please check our
<a href=https://arxiv.org/pdf/2003.02824.pdf target=_blank rel=noopener>papers</a> for more results.</p><hr><h2 id=videos>Videos</h2><p>Overview Introduction:<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/HxPYhOZco-4 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><p>CVPR'20 Presentation (1-min):<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/sKCXZksFOWA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><p>CVPR'20 Presentation (5-min):<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/16OTdVeKahs style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><p>WACV'20 Presentation:<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/RkdhHDvj4UA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><hr><h2 id=resources>Resources</h2><h4 id=papers--code>Papers & Code</h4><p><a href=https://github.com/cmhungsteve/SSTDA target=_blank rel=noopener><figure id=figure-codehttpsgithubcomcmhungstevesstda><img src=../../img/github_icon.png alt width=15%><figcaption><a href=https://github.com/cmhungsteve/SSTDA><strong>Code</strong></a></figcaption></figure></a></p><table><thead><tr><th><a href=https://arxiv.org/pdf/2003.02824.pdf target=_blank rel=noopener><figure id=figure-cvpr20httpsarxivorgpdf200302824pdf><img src=../../img/pdf_icon.png alt width=20%><figcaption><a href=https://arxiv.org/pdf/2003.02824.pdf>CVPR'20</a></figcaption></figure></a></th><th><a href=http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.pdf target=_blank rel=noopener><figure id=figure-wacv20httpopenaccessthecvfcomcontent_wacv_2020paperschen_action_segmentation_with_mixed_temporal_domain_adaptation_wacv_2020_paperpdf><img src=../../img/pdf_icon.png alt width=20%><figcaption><a href=http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.pdf>WACV'20</a></figcaption></figure></a></th></tr></thead></table><h4 id=presentations>Presentations</h4><table><thead><tr><th><a href="https://www.dropbox.com/s/4cwnwxsqp98mnwf/Oral_SSTDA_CVPR_2020_mute.pdf?dl=0" target=_blank rel=noopener><figure id=figure-slides-cvpr20><img data-src=/project/cdas/slides_CVPR2020_5min_hua8f37129a31f67c12b56cbe76af8ec8b_289280_2000x2000_fit_lanczos_2.png class=lazyload alt width=80% height=720><figcaption>Slides (CVPR'20)</figcaption></figure></a></th><th><a href="https://www.dropbox.com/s/bvkt581lk049zrl/Oral_MTDA_WACV_2020_mute.pdf?dl=0" target=_blank rel=noopener><figure id=figure-slides-wacv20><img data-src=/project/cdas/slides_WACV2020_hu90267c901349e116f78bd18d30269522_68620_2000x2000_fit_lanczos_2.png class=lazyload alt width=80% height=720><figcaption>Slides (WACV'20)</figcaption></figure></a></th></tr></thead></table><table><thead><tr><th><a href="https://www.dropbox.com/s/83sj8yf9b9a9qtq/CVPR2020_Steve_SSTDA_poster_v1.pdf?dl=0" target=_blank rel=noopener><figure id=figure-poster-cvpr20><a data-fancybox href=poster_CVPR2020_hucd8598a49fd7bde93d2d4b609b38c663_3490067_2000x2000_fit_lanczos_2.png data-caption="Poster (CVPR'20)"><img data-src=/project/cdas/poster_CVPR2020_hucd8598a49fd7bde93d2d4b609b38c663_3490067_2000x2000_fit_lanczos_2.png class=lazyload alt width=80% height=2256></a><figcaption>Poster (CVPR'20)</figcaption></figure></a></th><th><a href="https://www.dropbox.com/s/yzj25ok1ojb4qwn/WACV2020_Steve_MTDA_poster_v1.pdf?dl=0" target=_blank rel=noopener><figure id=figure-poster-wacv20><a data-fancybox href=poster_WACV2020_hu9313cdbd3861dca886f5c015f7c05644_2452412_2000x2000_fit_lanczos_2.png data-caption="Poster (WACV'20)"><img data-src=/project/cdas/poster_WACV2020_hu9313cdbd3861dca886f5c015f7c05644_2452412_2000x2000_fit_lanczos_2.png class=lazyload alt width=80% height=2256></a><figcaption>Poster (WACV'20)</figcaption></figure></a></th></tr></thead></table><h4 id=other-links>Other Links</h4><ul><li>CVF Open Access
[
<a href=http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Action_Segmentation_With_Joint_Self-Supervised_Temporal_Domain_Adaptation_CVPR_2020_paper.html target=_blank rel=noopener>CVPR'20</a> ]
[
<a href=http://openaccess.thecvf.com/content_WACV_2020/html/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.html target=_blank rel=noopener>WACV'20</a> ]</li><li>IEEE Xplore
[
<a href=https://ieeexplore.ieee.org/document/9157452 target=_blank rel=noopener>CVPR'20</a> ]
[
<a href=https://ieeexplore.ieee.org/document/9093535 target=_blank rel=noopener>WACV'20</a> ]</li><li>ML@GT [
<a href=https://mlgt-at-cvpr-2020.mailchimpsites.com/ target=_blank rel=noopener>Article</a> ]</li></ul><hr><h2 id=related-publications>Related Publications</h2><p>If you find this project useful, please cite our papers:</p><ul><li><a href=../../index.html target=_blank rel=noopener><strong>Min-Hung Chen</strong></a>, Baopu Li, Yingze Bao, Ghassan AlRegib, and Zsolt Kira, &ldquo;Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation&rdquo;,
<a href=http://cvpr2020.thecvf.com/ target=_blank rel=noopener><em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020</a>.</li><li><a href=../../index.html target=_blank rel=noopener><strong>Min-Hung Chen</strong></a>, Baopu Li, Yingze Bao, and Ghassan AlRegib, &ldquo;Action Segmentation with Mixed Temporal Domain Adaptation&rdquo;,
<a href=http://wacv20.wacv.net/ target=_blank rel=noopener><em>IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 2020</a>.</li></ul><h4 id=bibtex>BibTex</h4><pre><code>@inproceedings{chen2020action,
  title={Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation},
  author={Chen, Min-Hung and Li, Baopu and Bao, Yingze and AlRegib, Ghassan and Kira, Zsolt},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}

@inproceedings{chen2020mixed,
  title={Action Segmentation with Mixed Temporal Domain Adaptation},
  author={Chen, Min-Hung and Li, Baopu and Bao, Yingze and AlRegib, Ghassan},
  booktitle={IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2020}
}
</code></pre><hr><h2 id=members>Members</h2><p><strong><sup>1</sup>Georgia Institute of Technology   <sup>2</sup>Baidu USA<br></strong>*work done during an internship at Baidu USA</p><table><thead><tr><th><a href=../../index.html target=_blank rel=noopener><figure id=figure-min-hung-chensup1sup><img src=../../img/authors/head_me.jpg alt width=100%><figcaption>Min-Hung Chen<sup>1</sup>*</figcaption></figure></a></th><th><a href=https://dblp.org/pers/l/Li:Baopu.html target=_blank rel=noopener><figure id=figure-baopu-lisup2sup><img src=../../img/authors/head_bl.jpg alt width=100%><figcaption>Baopu Li<sup>2</sup></figcaption></figure></a></th><th><a href="https://scholar.google.com/citations?hl=en&amp;user=WiAQC68AAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target=_blank rel=noopener><figure id=figure-yingze-baosup2sup><img src=../../img/authors/head_yb.jpg alt width=100%><figcaption>Yingze Bao<sup>2</sup></figcaption></figure></a></th><th><a href=https://ghassanalregib.info/ target=_blank rel=noopener><figure id=figure-ghassan-alregibsup1sup><img src=../../img/authors/head_ga.jpg alt width=100%><figcaption>Ghassan AlRegib<sup>1</sup></figcaption></figure></a></th><th><a href=https://www.cc.gatech.edu/~zk15/ target=_blank rel=noopener><figure id=figure-zsolt-kirasup1sup><img src=../../img/authors/head_zk.jpg alt width=100%><figcaption>Zsolt Kira<sup>1</sup></figcaption></figure></a></th></tr></thead></table></div><div class=article-tags><a class="badge badge-light" href=../../tag/deep-learning/index.html>Deep Learning</a>
<a class="badge badge-light" href=../../tag/cvpr/index.html>CVPR</a>
<a class="badge badge-light" href=../../tag/action-segmentation/index.html>Action Segmentation</a>
<a class="badge badge-light" href=../../tag/domain-adaptation/index.html>Domain Adaptation</a>
<a class="badge badge-light" href=../../tag/wacv/index.html>WACV</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://minhungchen.netlify.app/project/cdas/&amp;text=Action%20Segmentation%20with%20Temporal%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://minhungchen.netlify.app/project/cdas/&amp;t=Action%20Segmentation%20with%20Temporal%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Action%20Segmentation%20with%20Temporal%20Domain%20Adaptation&body=https://minhungchen.netlify.app/project/cdas/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://minhungchen.netlify.app/project/cdas/&amp;title=Action%20Segmentation%20with%20Temporal%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Action%20Segmentation%20with%20Temporal%20Domain%20Adaptation%20https://minhungchen.netlify.app/project/cdas/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://minhungchen.netlify.app/project/cdas/&amp;title=Action%20Segmentation%20with%20Temporal%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=../../author/min-hung-chen/avatar_huc3acf39b4c2265ec309e042ecb270b7c_902720_270x270_fill_lanczos_center_2.png alt="Min-Hung Chen"><div class=media-body><h5 class=card-title><a href=../../index.html>Min-Hung Chen</a></h5><h6 class=card-subtitle>Research Engineer II</h6><p class=card-text>My research interest is Learning without Fully Supervision.</p><ul class=network-icon aria-hidden=true><li><a href=../../index.html#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/chensteven target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://github.com/cmhungsteve target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://twitter.com/CMHungSteven target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=ovzuxi8AAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=../../files/cv.pdf><i class="ai ai-cv"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=../../publication/mtda/index.html>Action Segmentation with Mixed Temporal Domain Adaptation</a></li><li><a href=../../publication/sstda/index.html>Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation</a></li><li><a href=../cdar/index.html>Temporal Attentive Alignment for Video Domain Adaptation</a></li><li><a href=../tslstm/index.html>Activity Recognition with RNN and Temporal-ConvNet</a></li><li><a href=../../talk/sstda_cvpr20/index.html>Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation</a></li></ul></div><div class="project-related-pages content-widget-hr"><h2>Publications</h2><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=../../publication/sstda/index.html>Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation</a></h3><div class=article-style>[<strong>CVPR 2020</strong>] Cross-domain action segmentation by aligning feature spaces across multiple temporal scales with self-supervised learning to reduce spatio-temporal variability.</div><div class="stream-meta article-metadata"><div><span><a href=../../author/min-hung-chen/index.html>Min-Hung Chen</a></span>, <span>Baopu Li</span>, <span>Yingze Bao</span>, <span>Ghassan AlRegib</span>, <span>Zsolt Kira</span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/pdf/2003.02824.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/sstda/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/cmhungsteve/SSTDA target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=index.html>Project</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.dropbox.com/s/83sj8yf9b9a9qtq/CVPR2020_Steve_SSTDA_poster_v1.pdf?dl=0" target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.dropbox.com/s/4cwnwxsqp98mnwf/Oral_SSTDA_CVPR_2020_mute.pdf?dl=0" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/CVPR42600.2020.00947 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/abs/2003.02824 target=_blank rel=noopener>ArXiv</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://youtu.be/HxPYhOZco-4 target=_blank rel=noopener>Overview Talk</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://youtu.be/sKCXZksFOWA target=_blank rel=noopener>1-min Video</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://youtu.be/16OTdVeKahs target=_blank rel=noopener>5-min Video</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Action_Segmentation_With_Joint_Self-Supervised_Temporal_Domain_Adaptation_CVPR_2020_paper.html target=_blank rel=noopener>CVF Open Access</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9157452 target=_blank rel=noopener>IEEE Xplore</a></div></div><div class=ml-3><a href=../../publication/sstda/index.html><img src=../../publication/sstda/featured_huadd658eeb4dd5c37780d1e61d3580e96_287352_150x0_resize_lanczos_2.png alt></a></div></div><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=../../publication/mtda/index.html>Action Segmentation with Mixed Temporal Domain Adaptation</a></h3><div class=article-style>[<strong>WACV 2020</strong>] Cross-domain action segmentation by aligning temporal feature spaces to reduce spatio-temporal variability.</div><div class="stream-meta article-metadata"><div><span><a href=../../author/min-hung-chen/index.html>Min-Hung Chen</a></span>, <span>Baopu Li</span>, <span>Yingze Bao</span>, <span>Ghassan AlRegib</span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/mtda/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=index.html>Project</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.dropbox.com/s/yzj25ok1ojb4qwn/WACV2020_Steve_MTDA_poster_v1.pdf?dl=0" target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.dropbox.com/s/bvkt581lk049zrl/Oral_MTDA_WACV_2020_mute.pdf?dl=0" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/WACV45572.2020.9093535 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://youtu.be/RkdhHDvj4UA target=_blank rel=noopener>Spotlight Video</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=http://openaccess.thecvf.com/content_WACV_2020/html/Chen_Action_Segmentation_with_Mixed_Temporal_Domain_Adaptation_WACV_2020_paper.html target=_blank rel=noopener>CVF Open Access</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9093535 target=_blank rel=noopener>IEEE Xplore</a></div></div><div class=ml-3><a href=../../publication/mtda/index.html><img src=../../publication/mtda/featured_hufbfb73f65e8d42e5936d6784f7547369_367915_150x0_resize_lanczos_2.png alt></a></div></div><h2>Talks</h2><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=../../talk/sstda_cvpr20/index.html>Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation</a></h3><div class=article-style>5-min invited presentation for the WebVision workshop at CVPR 2020</div><div class="stream-meta article-metadata"><div><span>Jun 15, 2020 10:41 AM &mdash; 10:45 AM</span>
<span class=middot-divider></span><span>Virtual</span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=index.html>Project</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.dropbox.com/s/4cwnwxsqp98mnwf/Oral_SSTDA_CVPR_2020_mute.pdf?dl=0" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://data.vision.ee.ethz.ch/cvl/webvision/ps-p2-cvpr.html target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../../talk/sstda_cvpr20/index.html><img src=../../talk/sstda_cvpr20/featured_hua8f37129a31f67c12b56cbe76af8ec8b_289280_150x0_resize_lanczos_2.png alt></a></div></div></div></div></article><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/python.min.js></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=../../../cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=../../js/academic.min.37431be2d92d7fb0160054761ab79602.js></script><div class=container><footer class=site-footer><p class=powered-by>© 2020 Min-Hung Chen</p><p class=powered-by>Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body>
<!-- Mirrored from minhungchen.netlify.app/project/cdas/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 04 Mar 2022 06:28:50 GMT -->
</html>