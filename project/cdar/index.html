<!doctype html><html lang=en-us>
<!-- Mirrored from minhungchen.netlify.app/project/cdar/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 04 Mar 2022 06:28:51 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Min-Hung Chen"><meta name=description content="Cross-domain action recognition with new datasets and novel video-based DA approaches."><link rel=alternate hreflang=en-us href=index.html><meta name=theme-color content="#2962ff"><script src=../../js/mathjax-config.js></script><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=../../../cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=../../../cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><script src=../../../cdn.jsdelivr.net/npm/mathjax%403/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&amp;display=swap"><link rel=stylesheet href=../../css/academic.min.ed35f6de5da3623407f7505649fd9d16.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-168846389-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
function trackOutboundLink(url,target){gtag('event','click',{'event_category':'outbound','event_label':url,'transport_type':'beacon','event_callback':function(){if(target!=='_blank'){document.location=url;}}});console.debug("Outbound link clicked: "+url);}
function onClickCallback(event){if((event.target.tagName!=='A')||(event.target.host===window.location.host)){return;}
trackOutboundLink(event.target,event.target.getAttribute('target'));}
gtag('js',new Date());gtag('config','UA-168846389-1',{});document.addEventListener('click',onClickCallback,false);</script><link rel=manifest href=../../index.webmanifest><link rel=icon type=image/png href=../../images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=../../images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png><link rel=canonical href=index.html><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@CMHungSteven"><meta property="twitter:creator" content="@CMHungSteven"><meta property="og:site_name" content="Min-Hung Chen"><meta property="og:url" content="https://minhungchen.netlify.app/project/cdar/"><meta property="og:title" content="Temporal Attentive Alignment for Video Domain Adaptation | Min-Hung Chen"><meta property="og:description" content="Cross-domain action recognition with new datasets and novel video-based DA approaches."><meta property="og:image" content="https://minhungchen.netlify.app/project/cdar/featured.png"><meta property="twitter:image" content="https://minhungchen.netlify.app/project/cdar/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-06-25T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-16T11:33:34+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://minhungchen.netlify.app/project/cdar/"},"headline":"Temporal Attentive Alignment for Video Domain Adaptation","image":["https://minhungchen.netlify.app/project/cdar/featured.png"],"datePublished":"2020-06-25T00:00:00Z","dateModified":"2021-05-16T11:33:34+08:00","author":{"@type":"Person","name":"Min-Hung Chen"},"publisher":{"@type":"Organization","name":"Min-Hung Chen","logo":{"@type":"ImageObject","url":"https://minhungchen.netlify.app/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"}},"description":"Cross-domain action recognition with new datasets and novel video-based DA approaches."}</script><title>Temporal Attentive Alignment for Video Domain Adaptation | Min-Hung Chen</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=../../index.html>Min-Hung Chen</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=../../index.html>Min-Hung Chen</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=../../index.html#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#news><span>News</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#activities><span>Activities</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#honors><span>Honors</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#teaching><span>Teaching</span></a></li><li class=nav-item><a class=nav-link href=../../index.html#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class="nav-link js-theme-selector" data-toggle=dropdown aria-haspopup=true><i class="fas fa-palette" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav><article class="article article-project"><div class="article-container pt-3"><h1>Temporal Attentive Alignment for Video Domain Adaptation</h1><div class=article-metadata><div><span><a href=../../author/min-hung-chen/index.html>Min-Hung Chen</a></span>, <span>Zsolt Kira</span>, <span>Ghassan AlRegib</span>, <span>Jaekwon Yoo</span>, <span>Ruxin Chen</span>, <span>Jian Zheng</span></div><span class=article-date>Last updated on
May 16, 2021</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=../../publication/TA3N/index-2.html>ICCV'19 (Oral)</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://arxiv.org/abs/1905.10861 target=_blank rel=noopener>CVPRW'19</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:522px><div style=position:relative><img src=featured_hud7a8fc5dad65e6ab5197f1b936b10b1b_267681_720x0_resize_lanczos_2.png alt="Overall illustration for cross-domain action recognition." class=featured-image></div></div><div class=article-container><div class=article-style><h2 id=motivation>Motivation</h2><p>Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Furthermore, there do not exist well-organized datasets to evaluate and benchmark the performance of DA algorithms for videos. Therefore, new datasets and approaches for video DA are desired.</p><hr><h2 id=challenges>Challenges</h2><p>Videos can suffer from domain shift along both the spatial and temporal directions, bringing the need of alignment for embedded feature spaces along both directions. However, most DA approaches focus on spatial direction only.</p><hr><h2 id=our-approaches>Our Approaches</h2><p>We propose <strong>Temporal Attentive Adversarial Adaptation Network (TA<sup>3</sup>N)</strong> to simultaneously attend, align and learn temporal dynamics across domains. TA<sup>3</sup>N contains the following components:</p><ul><li><em>TemRelation module</em>: learn various local temporal features embedded with relation information in different temporal scales</li><li><em>Domain attention mechanism</em>: align local features with larger domain discrepancy (i.e. contribute more to the overall domain shift)<ul><li>Assign local features with attention weights calculated using domain entropy</li></ul></li></ul><figure id=figure-the-domain-attention-mechanism-in-tasup3supn-thicker-arrows-correspond-to-larger-attention-weights><a data-fancybox href=approach_TA3N_huff70fa82cfaf62901a8cb74823d96b36_122314_2000x2000_fit_lanczos_2.png data-caption="The domain attention mechanism in TA3N (Thicker arrows correspond to larger attention weights)."><img data-src=/project/cdar/approach_TA3N_huff70fa82cfaf62901a8cb74823d96b36_122314_2000x2000_fit_lanczos_2.png class=lazyload alt width=100% height=346></a><figcaption>The domain attention mechanism in TA<sup>3</sup>N (Thicker arrows correspond to larger attention weights).</figcaption></figure><hr><h2 id=results>Results</h2><p>We evaluate both small- and large-scale datasets, and achieve the state-of-the-art performance:</p><ul><li>Quantitative results:<figure id=figure-the-comparison-of-accuracy--with-other-approaches-on-_ucf-hmdbsubfullsub_-ucf----hmdb><a data-fancybox href=result_TA3N_quantitative_hu86af7cd341c815527e27bee7cb48f53f_82705_2000x2000_fit_lanczos_2.png data-caption="The comparison of accuracy (%) with other approaches on UCF-HMDBfull (UCF &ndash;> HMDB)."><img data-src=/project/cdar/result_TA3N_quantitative_hu86af7cd341c815527e27bee7cb48f53f_82705_2000x2000_fit_lanczos_2.png class=lazyload alt width=50% height=360></a><figcaption>The comparison of accuracy (%) with other approaches on <em>UCF-HMDB<sub>full</sub></em> (UCF &ndash;> HMDB).</figcaption></figure></li><li>Qualitative results:<figure id=figure-the-comparison-of-t-sne-visualization-between-image-based-da-and-our-approach-blue-source-orange-target><a data-fancybox href=result_TA3N_qualitative_hu52c64adb84bea465dcafc27d3d518cd8_157293_2000x2000_fit_lanczos_2.png data-caption="The comparison of t-SNE visualization between image-based DA and our approach (blue: source, orange: target)."><img data-src=/project/cdar/result_TA3N_qualitative_hu52c64adb84bea465dcafc27d3d518cd8_157293_2000x2000_fit_lanczos_2.png class=lazyload alt width=80% height=391></a><figcaption>The comparison of t-SNE visualization between image-based DA and our approach (blue: source, orange: target).</figcaption></figure></li></ul><p>Please check our
<a href=https://arxiv.org/pdf/1907.12743.pdf target=_blank rel=noopener>paper</a> for more results.</p><hr><h2 id=videos>Videos</h2><p>ICCV'19 Oral Presentation (please turn on closed captions):<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/j9cDuzmpYP8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><p>ICCV'19 Oral Presentation (officially recorded):<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src="https://www.youtube.com/embed/8oUPyhwzIDo?start=6100&amp;end=6395" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><hr><h2 id=resources>Resources</h2><h4 id=papers--code>Papers & Code</h4><p><a href=https://github.com/cmhungsteve/TA3N target=_blank rel=noopener><figure id=figure-codehttpsgithubcomcmhungsteveta3n><img src=../../img/github_icon.png alt width=15%><figcaption><a href=https://github.com/cmhungsteve/TA3N><strong>Code</strong></a></figcaption></figure></a></p><table><thead><tr><th><a href=https://arxiv.org/pdf/1907.12743.pdf target=_blank rel=noopener><figure id=figure-iccv19httpsarxivorgpdf190712743pdf><img src=../../img/pdf_icon.png alt width=20%><figcaption><a href=https://arxiv.org/pdf/1907.12743.pdf>ICCV'19</a></figcaption></figure></a></th><th><a href=https://arxiv.org/pdf/1905.10861.pdf target=_blank rel=noopener><figure id=figure-cvprw19httpsarxivorgpdf190510861pdf><img src=../../img/pdf_icon.png alt width=20%><figcaption><a href=https://arxiv.org/pdf/1905.10861.pdf>CVPRW'19</a></figcaption></figure></a></th></tr></thead></table><h4 id=presentations>Presentations</h4><table><thead><tr><th><a href="https://www.dropbox.com/s/7p1u8yro9hqseac/Oral_TA3N_ICCV_2019_mute.pdf?dl=0" target=_blank rel=noopener><figure id=figure-slides-iccv19><img data-src=/project/cdar/slides_ICCV2019_hubefd42348ee7371b35905da2500f1e83_82332_2000x2000_fit_lanczos_2.png class=lazyload alt width=100% height=720><figcaption>Slides (ICCV'19)</figcaption></figure></a></th><th><a href="https://www.dropbox.com/s/nsb9bdfaz03mjv2/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0" target=_blank rel=noopener><figure id=figure-poster-iccv19><a data-fancybox href=poster_ICCV2019_hub6005853782b9081f07ee0575514bfea_1443480_2000x2000_fit_lanczos_2.png data-caption="Poster (ICCV'19)"><img data-src=/project/cdar/poster_ICCV2019_hub6005853782b9081f07ee0575514bfea_1443480_2000x2000_fit_lanczos_2.png class=lazyload alt width=70% height=1776></a><figcaption>Poster (ICCV'19)</figcaption></figure></a></th></tr></thead></table><h4 id=other-links>Other Links</h4><ul><li>CVF Open Access
[
<a href=http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.html target=_blank rel=noopener>ICCV'19</a> ]</li><li>IEEE Xplore
[
<a href=https://ieeexplore.ieee.org/document/9008391 target=_blank rel=noopener>ICCV'19</a> ]</li><li>The workshop on Learning from Unlabeled Videos (LUV)
[
<a href=https://sites.google.com/view/luv2019/program target=_blank rel=noopener>CVPRW'19</a> ]</li><li>ML@GT [
<a href=https://mlatgt.blog/2019/09/10/overcoming-large-scale-annotation-requirements-for-understanding-videos-in-the-wild target=_blank rel=noopener>Blog</a> ][
<a href=https://mailchi.mp/4ed0cbf6a67d/iccv2019 target=_blank rel=noopener>Article</a> ]</li></ul><h4 id=datasets>Datasets</h4><p>We propose two large-scale cross-domain action recognition datasets, <em>UCF-HMDB<sub>full</sub></em> and <em>Kinetics-Gameplay</em>.</p><p><figure id=figure-the-snapshots-in-the-_ucf-hmdbsubfullsub_-dataset><a data-fancybox href=ucf_hmdb_hu5f00b314049d434c65c8e828bec18a41_384767_2000x2000_fit_lanczos_2.png data-caption="The snapshots in the UCF-HMDBfull dataset."><img data-src=/project/cdar/ucf_hmdb_hu5f00b314049d434c65c8e828bec18a41_384767_2000x2000_fit_lanczos_2.png class=lazyload alt width=589 height=50%></a><figcaption>The snapshots in the <em>UCF-HMDB<sub>full</sub></em> dataset.</figcaption></figure><figure id=figure-the-snapshots-in-the-_kinetics-gameplay_-dataset><a data-fancybox href=kinetics_gameplay_hu809b38b434046d8cf813c72d107bbaef_462778_2000x2000_fit_lanczos_2.png data-caption="The snapshots in the Kinetics-Gameplay dataset."><img data-src=/project/cdar/kinetics_gameplay_hu809b38b434046d8cf813c72d107bbaef_462778_2000x2000_fit_lanczos_2.png class=lazyload alt width=791 height=50%></a><figcaption>The snapshots in the <em>Kinetics-Gameplay</em> dataset.</figcaption></figure></p><p>To download the dataset, please visit our
<a href=https://github.com/cmhungsteve/TA3N#dataset-preparation target=_blank rel=noopener>GitHub</a>.<br>Feel free to check our
<a href=https://arxiv.org/pdf/1907.12743.pdf target=_blank rel=noopener>paper</a> for more dataset details.</p><hr><h2 id=related-publications>Related Publications</h2><p>If you find this project useful, please cite our papers:</p><ul><li><a href=../../index.html target=_blank rel=noopener><strong>Min-Hung Chen</strong></a>, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen, and Jian Zheng, &ldquo;Temporal Attentive Alignment for Large-Scale Video Domain Adaptation&rdquo;,
<a href=http://iccv2019.thecvf.com/ target=_blank rel=noopener><em>IEEE International Conference on Computer Vision (ICCV)</em>, 2019</a> <strong>[Oral (acceptance rate: 4.6%), travel grant awarded]</strong>.</li><li><a href=../../index.html target=_blank rel=noopener><strong>Min-Hung Chen</strong></a>, Zsolt Kira, and Ghassan AlRegib, &ldquo;Temporal Attentive Alignment for Video Domain Adaptation&rdquo;,
<a href=https://sites.google.com/view/luv2019 target=_blank rel=noopener><em>CVPR Workshop on Learning from Unlabeled Videos (LUV)</em>, 2019</a>.</li></ul><h4 id=bibtex>BibTex</h4><pre><code>@inproceedings{chen2019temporal,
  title={Temporal attentive alignment for large-scale video domain adaptation},
  author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan and Yoo, Jaekwon and Chen, Ruxin and Zheng, Jian},
  booktitle={IEEE International Conference on Computer Vision (ICCV)},
  year={2019}
}

@article{chen2019taaan,
  title={Temporal Attentive Alignment for Video Domain Adaptation},
  author={Chen, Min-Hung and Kira, Zsolt and AlRegib, Ghassan},
  journal={CVPR Workshop on Learning from Unlabeled Videos},
  year={2019},
  url={https://arxiv.org/abs/1905.10861}
}
</code></pre><hr><h2 id=members>Members</h2><p><strong><sup>1</sup>Georgia Institute of Technology   <sup>2</sup>Sony Interactive Entertainment LLC   <sup>3</sup>Binghamton University<br></strong>*work partially done as a SIE intern</p><table><thead><tr><th><a href=../../index.html target=_blank rel=noopener><figure id=figure-min-hung-chensup1sup><img src=../../img/authors/head_me.jpg alt width=100%><figcaption>Min-Hung Chen<sup>1</sup>*</figcaption></figure></a></th><th><a href=https://www.cc.gatech.edu/~zk15/ target=_blank rel=noopener><figure id=figure-zsolt-kirasup1sup><img src=../../img/authors/head_zk.jpg alt width=100%><figcaption>Zsolt Kira<sup>1</sup></figcaption></figure></a></th><th><a href=https://ghassanalregib.info/ target=_blank rel=noopener><figure id=figure-ghassan-alregibsup1sup><img src=../../img/authors/head_ga.jpg alt width=100%><figcaption>Ghassan AlRegib<sup>1</sup></figcaption></figure></a></th><th><a href=https://www.linkedin.com/in/jaekwon-yoo-8685862b/ target=_blank rel=noopener><figure id=figure-jaekwon-yoosup2sup><img src=../../img/authors/head_unknown.jpg alt width=100%><figcaption>Jaekwon Yoo<sup>2</sup></figcaption></figure></a></th><th><a href=https://www.linkedin.com/in/ruxin-chen-991477119/ target=_blank rel=noopener><figure id=figure-ruxin-chensup2sup><img src=../../img/authors/head_unknown.jpg alt width=100%><figcaption>Ruxin Chen<sup>2</sup></figcaption></figure></a></th><th><a href="https://scholar.google.com/citations?hl=en&amp;user=5YR6dTEAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target=_blank rel=noopener><figure id=figure-jian-zhengsup3sup><img src=../../img/authors/head_jz.jpg alt width=100%><figcaption>Jian Zheng<sup>3</sup>*</figcaption></figure></a></th></tr></thead></table></div><div class=article-tags><a class="badge badge-light" href=../../tag/action-recognition/index.html>Action Recognition</a>
<a class="badge badge-light" href=../../tag/deep-learning/index.html>Deep Learning</a>
<a class="badge badge-light" href=../../tag/datasets/index.html>Datasets</a>
<a class="badge badge-light" href=../../tag/domain-adaptation/index.html>Domain Adaptation</a>
<a class="badge badge-light" href=../../tag/iccv/index.html>ICCV</a>
<a class="badge badge-light" href=../../tag/oral/index.html>Oral</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://minhungchen.netlify.app/project/cdar/&amp;text=Temporal%20Attentive%20Alignment%20for%20Video%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://minhungchen.netlify.app/project/cdar/&amp;t=Temporal%20Attentive%20Alignment%20for%20Video%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Temporal%20Attentive%20Alignment%20for%20Video%20Domain%20Adaptation&body=https://minhungchen.netlify.app/project/cdar/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://minhungchen.netlify.app/project/cdar/&amp;title=Temporal%20Attentive%20Alignment%20for%20Video%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Temporal%20Attentive%20Alignment%20for%20Video%20Domain%20Adaptation%20https://minhungchen.netlify.app/project/cdar/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://minhungchen.netlify.app/project/cdar/&amp;title=Temporal%20Attentive%20Alignment%20for%20Video%20Domain%20Adaptation" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=../../author/min-hung-chen/avatar_huc3acf39b4c2265ec309e042ecb270b7c_902720_270x270_fill_lanczos_center_2.png alt="Min-Hung Chen"><div class=media-body><h5 class=card-title><a href=../../index.html>Min-Hung Chen</a></h5><h6 class=card-subtitle>Research Engineer II</h6><p class=card-text>My research interest is Learning without Fully Supervision.</p><ul class=network-icon aria-hidden=true><li><a href=../../index.html#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/chensteven target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://github.com/cmhungsteve target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://twitter.com/CMHungSteven target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=ovzuxi8AAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=../../files/cv.pdf><i class="ai ai-cv"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=../../publication/ta3n/index.html>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</a></li><li><a href=../cdas/index.html>Action Segmentation with Temporal Domain Adaptation</a></li><li><a href=../tslstm/index.html>Activity Recognition with RNN and Temporal-ConvNet</a></li><li><a href=../../talk/ta3n_iccv19/index.html>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</a></li><li><a href=../curetsd/index.html>Traffic Sign Detection under Challenging Conditions</a></li></ul></div><div class="project-related-pages content-widget-hr"><h2>Publications</h2><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=../../publication/ta3n/index.html>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</a></h3><div class=article-style>[<strong>ICCV 2019 (Oral)</strong>] Cross-domain action recognition with new datasets and novel attention-based DA approaches.</div><div class="stream-meta article-metadata"><div><span><a href=../../author/min-hung-chen/index.html>Min-Hung Chen</a></span>, <span>Zsolt Kira</span>, <span>Ghassan AlRegib</span>, <span>Jaekwon Yoo</span>, <span>Ruxin Chen</span>, <span>Jian Zheng</span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/pdf/1907.12743.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/ta3n/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/cmhungsteve/TA3N target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/cmhungsteve/TA3N#dataset-preparation target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=index.html>Project</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.dropbox.com/s/nsb9bdfaz03mjv2/ICCV2019_Steve_TA3N_poster_v1_2.pdf?dl=0" target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.dropbox.com/s/7p1u8yro9hqseac/Oral_TA3N_ICCV_2019_mute.pdf?dl=0" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/ICCV.2019.00642 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/abs/1907.12743 target=_blank rel=noopener>ArXiv</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://mlatgt.blog/2019/09/10/overcoming-large-scale-annotation-requirements-for-understanding-videos-in-the-wild target=_blank rel=noopener>Blog</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=../../talk/TA3N/index.html>Talk</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://youtu.be/j9cDuzmpYP8 target=_blank rel=noopener>Oral Video</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.html target=_blank rel=noopener>CVF Open Access</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://ieeexplore.ieee.org/document/9008391 target=_blank rel=noopener>IEEE Xplore</a></div></div><div class=ml-3><a href=../../publication/ta3n/index.html><img src=../../publication/ta3n/featured_hu4b8eb3f05a172a517631a462dd5558a4_261816_150x0_resize_lanczos_2.png alt></a></div></div><h2>Talks</h2><div class="media stream-item"><div class=media-body><h3 class="article-title mb-0 mt-0"><a href=../../talk/ta3n_iccv19/index.html>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</a></h3><div class=article-style>5-min video for the Oral presentation in ICCV 2019</div><div class="stream-meta article-metadata"><div><span>Oct 31, 2019 3:05 PM &mdash; 3:10 PM</span>
<span class=middot-divider></span><span>COEX Convention Center</span></div></div><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=index.html>Project</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.dropbox.com/s/7p1u8yro9hqseac/Oral_TA3N_ICCV_2019_mute.pdf?dl=0" target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://conftube.com/video/8oUPyhwzIDo?tocitem=146" target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=../../talk/ta3n_iccv19/index.html><img src=../../talk/ta3n_iccv19/featured_hu61d47f03780ebd00c1d3cca67daa4647_384005_150x0_resize_lanczos_2.png alt></a></div></div></div></div></article><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/python.min.js></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=../../../cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=../../../cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=../../js/academic.min.37431be2d92d7fb0160054761ab79602.js></script><div class=container><footer class=site-footer><p class=powered-by>© 2020 Min-Hung Chen</p><p class=powered-by>Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body>
<!-- Mirrored from minhungchen.netlify.app/project/cdar/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 04 Mar 2022 06:28:59 GMT -->
</html>